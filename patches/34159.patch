From 8e29d0fc66c87869f1b1a63bf551899daca95d46 Mon Sep 17 00:00:00 2001
From: Connor Abbott <cwabbott0@gmail.com>
Date: Fri, 14 Mar 2025 22:20:13 +0100
Subject: [PATCH 01/12] tu: Fix size of frag_size_ir3 and frag_offset_ir3
 driver params

They are an array, so we have to reserve extra space for extra views.
This bug was being masked by the bug fixed in the next commit.

Fixes: 76e417ca593 ("turnip,ir3/a750: Implement consts loading via preamble")
---
 src/freedreno/ir3/ir3_nir.c | 6 +++++-
 src/freedreno/ir3/ir3_nir.h | 1 +
 2 files changed, 6 insertions(+), 1 deletion(-)

diff --git a/src/freedreno/ir3/ir3_nir.c b/src/freedreno/ir3/ir3_nir.c
index 8a2ad6afda89c..587335084e5ae 100644
--- a/src/freedreno/ir3/ir3_nir.c
+++ b/src/freedreno/ir3/ir3_nir.c
@@ -1234,6 +1234,7 @@ bool
 ir3_get_driver_param_info(const nir_shader *shader, nir_intrinsic_instr *intr,
                           struct driver_param_info *param_info)
 {
+   param_info->extra_size = 0;
    switch (intr->intrinsic) {
    case nir_intrinsic_load_base_workgroup_id:
       param_info->offset = IR3_DP_CS(base_group_x);
@@ -1285,9 +1286,11 @@ ir3_get_driver_param_info(const nir_shader *shader, nir_intrinsic_instr *intr,
       break;
    case nir_intrinsic_load_frag_size_ir3:
       param_info->offset = IR3_DP_FS(frag_size);
+      param_info->extra_size = 4 * (nir_intrinsic_range(intr) - 1);
       break;
    case nir_intrinsic_load_frag_offset_ir3:
       param_info->offset = IR3_DP_FS(frag_offset);
+      param_info->extra_size = 4 * (nir_intrinsic_range(intr) - 1);
       break;
    case nir_intrinsic_load_frag_invocation_count:
       param_info->offset = IR3_DP_FS(frag_invocation_count);
@@ -1344,7 +1347,8 @@ ir3_nir_scan_driver_consts(struct ir3_compiler *compiler, nir_shader *shader,
             if (ir3_get_driver_param_info(shader, intr, &param_info)) {
                num_driver_params =
                   MAX2(num_driver_params,
-                       param_info.offset + nir_intrinsic_dest_components(intr));
+                       param_info.offset + param_info.extra_size +
+                       nir_intrinsic_dest_components(intr));
             }
          }
       }
diff --git a/src/freedreno/ir3/ir3_nir.h b/src/freedreno/ir3/ir3_nir.h
index 0f826f74b898f..1c7e507e2775a 100644
--- a/src/freedreno/ir3/ir3_nir.h
+++ b/src/freedreno/ir3/ir3_nir.h
@@ -139,6 +139,7 @@ nir_def *ir3_rematerialize_def_for_preamble(nir_builder *b, nir_def *def,
 
 struct driver_param_info {
    uint32_t offset;
+   uint32_t extra_size;
 };
 
 bool ir3_get_driver_param_info(const nir_shader *shader,
-- 
GitLab


From f983c40879e1514f5e36a0bf1dd938e7cd139a07 Mon Sep 17 00:00:00 2001
From: Connor Abbott <cwabbott0@gmail.com>
Date: Mon, 10 Mar 2025 16:44:22 -0400
Subject: [PATCH 02/12] tu: Fix reported FDM fragment size with multiview

We were never setting has_multiview. It's not actually necessary anyway,
since we can just do the optimization we were trying to do whenever
num_views is 1 instead.

This doesn't affect the actual fragment size, which was already correct,
only gl_FragSizeEXT.

Fixes: 6f2be52487b ("tu, ir3: Handle FDM shader builtins")
Part-of: <https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/33991>
---
 src/freedreno/vulkan/tu_shader.cc | 3 +--
 1 file changed, 1 insertion(+), 2 deletions(-)

diff --git a/src/freedreno/vulkan/tu_shader.cc b/src/freedreno/vulkan/tu_shader.cc
index f54de781a6a51..0f7d31077351b 100644
--- a/src/freedreno/vulkan/tu_shader.cc
+++ b/src/freedreno/vulkan/tu_shader.cc
@@ -1028,7 +1028,6 @@ tu_lower_io(nir_shader *shader, struct tu_device *dev,
 struct lower_fdm_options {
    unsigned num_views;
    bool adjust_fragcoord;
-   bool multiview;
 };
 
 static bool
@@ -1055,7 +1054,7 @@ lower_fdm_instr(struct nir_builder *b, nir_instr *instr, void *data)
    nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
 
    nir_def *view;
-   if (options->multiview) {
+   if (options->num_views > 1) {
       nir_variable *view_var =
          nir_find_variable_with_location(b->shader, nir_var_shader_in,
                                          VARYING_SLOT_VIEW_INDEX);
-- 
GitLab


From 68e562bdf1f220b2229f942c72b0b2659fecc869 Mon Sep 17 00:00:00 2001
From: Connor Abbott <cwabbott0@gmail.com>
Date: Thu, 13 Mar 2025 09:40:53 -0400
Subject: [PATCH 03/12] tu: Only allow power-of-two fragment areas

Non-power-of-two fragment areas can result in precision loss and missed
fragments, which was seen in an upcoming CTS test.
---
 src/freedreno/vulkan/tu_cmd_buffer.cc | 23 +++++++++++++++++------
 1 file changed, 17 insertions(+), 6 deletions(-)

diff --git a/src/freedreno/vulkan/tu_cmd_buffer.cc b/src/freedreno/vulkan/tu_cmd_buffer.cc
index 2652eb195eecb..8d8a59fc2aae9 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.cc
+++ b/src/freedreno/vulkan/tu_cmd_buffer.cc
@@ -2358,8 +2358,16 @@ tu_calc_frag_area(struct tu_cmd_buffer *cmd,
          else
             floor_y += 1.f;
       }
-      tile->frag_areas[i].width = floor_x;
-      tile->frag_areas[i].height = floor_y;
+      uint32_t width = floor_x;
+      uint32_t height = floor_y;
+
+      /* Areas that aren't a power of two, especially large areas, can create
+       * in floating-point rounding errors when dividing by the area in the
+       * viewport that result in under-rendering. Round down to a power of two
+       * to make sure all operations are exact.
+       */
+      width = 1u << util_logbase2(width);
+      height = 1u << util_logbase2(height);
 
       /* Make sure that the width/height divides the tile width/height so
        * we don't have to do extra awkward clamping of the edges of each
@@ -2368,10 +2376,13 @@ tu_calc_frag_area(struct tu_cmd_buffer *cmd,
        *
        * TODO: Try to take advantage of the total area allowance here, too.
        */
-      while (tiling->tile0.width % tile->frag_areas[i].width != 0)
-         tile->frag_areas[i].width--;
-      while (tiling->tile0.height % tile->frag_areas[i].height != 0)
-         tile->frag_areas[i].height--;
+      while (tiling->tile0.width % width != 0)
+         width /= 2;
+      while (tiling->tile0.height % height != 0)
+         height /= 2;
+
+      tile->frag_areas[i].width = width;
+      tile->frag_areas[i].height = height;
    }
 
    /* If at any point we were forced to use the same scaling for all
-- 
GitLab


From 60c2101a495ed839f6aff3d8dfa4e5ec739ed599 Mon Sep 17 00:00:00 2001
From: Connor Abbott <cwabbott0@gmail.com>
Date: Wed, 29 Jan 2025 10:18:14 -0500
Subject: [PATCH 04/12] tu: Split out part of tiling config to vsc config

For FDM offset, we will need to expand the number of bins by 1, which
can change how pipes are allocated. We don't necessarily know whether
FDM offset will be used when creating the VkFramebuffer, so we'll have
to create two different configs when FDM is enabled. Split out the parts
that are affected by the number of bins into a separate "VSC config"
struct that will be duplicated with FDM offset.
---
 src/freedreno/vulkan/tu_cmd_buffer.cc  |  60 ++++++++------
 src/freedreno/vulkan/tu_device.h       |  22 ++---
 src/freedreno/vulkan/tu_tracepoints.py |   2 +-
 src/freedreno/vulkan/tu_util.cc        | 107 +++++++++++++------------
 4 files changed, 105 insertions(+), 86 deletions(-)

diff --git a/src/freedreno/vulkan/tu_cmd_buffer.cc b/src/freedreno/vulkan/tu_cmd_buffer.cc
index 8d8a59fc2aae9..669f72351de5d 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.cc
+++ b/src/freedreno/vulkan/tu_cmd_buffer.cc
@@ -955,6 +955,7 @@ use_hw_binning(struct tu_cmd_buffer *cmd)
 {
    const struct tu_framebuffer *fb = cmd->state.framebuffer;
    const struct tu_tiling_config *tiling = &fb->tiling[cmd->state.gmem_layout];
+   const struct tu_vsc_config *vsc = &tiling->vsc;
 
    /* XFB commands are emitted for BINNING || SYSMEM, which makes it
     * incompatible with non-hw binning GMEM rendering. this is required because
@@ -963,7 +964,7 @@ use_hw_binning(struct tu_cmd_buffer *cmd)
     * XFB was used.
     */
    if (cmd->state.rp.xfb_used) {
-      assert(tiling->binning_possible);
+      assert(vsc->binning_possible);
       return true;
    }
 
@@ -974,11 +975,11 @@ use_hw_binning(struct tu_cmd_buffer *cmd)
     */
    if (cmd->state.rp.has_prim_generated_query_in_rp ||
        cmd->state.prim_generated_query_running_before_rp) {
-      assert(tiling->binning_possible);
+      assert(vsc->binning_possible);
       return true;
    }
 
-   return tiling->binning;
+   return vsc->binning;
 }
 
 static bool
@@ -1018,8 +1019,10 @@ use_sysmem_rendering(struct tu_cmd_buffer *cmd,
       return true;
    }
 
+   const struct tu_vsc_config *vsc = &cmd->state.tiling->vsc;
+
    /* XFB is incompatible with non-hw binning GMEM rendering, see use_hw_binning */
-   if (cmd->state.rp.xfb_used && !cmd->state.tiling->binning_possible) {
+   if (cmd->state.rp.xfb_used && !vsc->binning_possible) {
       cmd->state.rp.gmem_disable_reason =
          "XFB is incompatible with non-hw binning GMEM rendering";
       return true;
@@ -1030,7 +1033,7 @@ use_sysmem_rendering(struct tu_cmd_buffer *cmd,
     */
    if ((cmd->state.rp.has_prim_generated_query_in_rp ||
         cmd->state.prim_generated_query_running_before_rp) &&
-       !cmd->state.tiling->binning_possible) {
+       !vsc->binning_possible) {
       cmd->state.rp.gmem_disable_reason =
          "QUERY_TYPE_PRIMITIVES_GENERATED is incompatible with non-hw binning GMEM rendering";
       return true;
@@ -1061,7 +1064,9 @@ static void
 tu6_emit_cond_for_load_stores(struct tu_cmd_buffer *cmd, struct tu_cs *cs,
                               uint32_t pipe, uint32_t slot, bool skip_wfm)
 {
-   if (cmd->state.tiling->binning_possible &&
+   const struct tu_vsc_config *vsc = &cmd->state.tiling->vsc;
+
+   if (vsc->binning_possible &&
        cmd->state.pass->has_cond_load_store) {
       tu_cs_emit_pkt7(cs, CP_REG_TEST, 1);
       tu_cs_emit(cs, A6XX_CP_REG_TEST_0_REG(REG_A6XX_VSC_STATE_REG(pipe)) |
@@ -1089,6 +1094,7 @@ tu6_emit_tile_select(struct tu_cmd_buffer *cmd,
 {
    struct tu_physical_device *phys_dev = cmd->device->physical_device;
    const struct tu_tiling_config *tiling = cmd->state.tiling;
+   const struct tu_vsc_config *vsc = &tiling->vsc;
    bool hw_binning = use_hw_binning(cmd);
 
    tu_cs_emit_pkt7(cs, CP_SET_MARKER, 1);
@@ -1133,7 +1139,7 @@ tu6_emit_tile_select(struct tu_cmd_buffer *cmd,
       tu_cs_emit(cs, 0x0);
 
       tu_cs_emit_pkt7(cs, CP_SET_BIN_DATA5_OFFSET, abs_mask ? 5 : 4);
-      tu_cs_emit(cs, tiling->pipe_sizes[tile->pipe] |
+      tu_cs_emit(cs, vsc->pipe_sizes[tile->pipe] |
                      CP_SET_BIN_DATA5_0_VSC_N(slot) |
                      CP_SET_BIN_DATA5_0_VSC_MASK(tile->slot_mask >> slot) |
                      COND(abs_mask, CP_SET_BIN_DATA5_0_ABS_MASK(ABS_MASK)));
@@ -1251,6 +1257,7 @@ tu6_emit_tile_store(struct tu_cmd_buffer *cmd, struct tu_cs *cs)
    const struct tu_render_pass *pass = cmd->state.pass;
    const struct tu_subpass *subpass = &pass->subpasses[pass->subpass_count-1];
    const struct tu_framebuffer *fb = cmd->state.framebuffer;
+   const struct tu_vsc_config *vsc = &cmd->state.tiling->vsc;
 
    if (pass->has_fdm)
       tu_cs_set_writeable(cs, true);
@@ -1279,7 +1286,7 @@ tu6_emit_tile_store(struct tu_cmd_buffer *cmd, struct tu_cs *cs)
 
    for (uint32_t a = 0; a < pass->attachment_count; ++a) {
       if (pass->attachments[a].gmem) {
-         const bool cond_exec_allowed = cmd->state.tiling->binning_possible &&
+         const bool cond_exec_allowed = vsc->binning_possible &&
                                         cmd->state.pass->has_cond_load_store;
          tu_store_gmem_attachment<CHIP>(cmd, cs, &resolve_group, a, a,
                                   fb->layers, subpass->multiview_mask,
@@ -1650,17 +1657,18 @@ update_vsc_pipe(struct tu_cmd_buffer *cmd,
                 uint32_t num_vsc_pipes)
 {
    const struct tu_tiling_config *tiling = cmd->state.tiling;
+   const struct tu_vsc_config *vsc = &tiling->vsc;
 
    tu_cs_emit_regs(cs,
                    A6XX_VSC_BIN_SIZE(.width = tiling->tile0.width,
                                      .height = tiling->tile0.height));
 
    tu_cs_emit_regs(cs,
-                   A6XX_VSC_BIN_COUNT(.nx = tiling->tile_count.width,
-                                      .ny = tiling->tile_count.height));
+                   A6XX_VSC_BIN_COUNT(.nx = vsc->tile_count.width,
+                                      .ny = vsc->tile_count.height));
 
    tu_cs_emit_pkt4(cs, REG_A6XX_VSC_PIPE_CONFIG_REG(0), num_vsc_pipes);
-   tu_cs_emit_array(cs, tiling->pipe_config, num_vsc_pipes);
+   tu_cs_emit_array(cs, vsc->pipe_config, num_vsc_pipes);
 
    tu_cs_emit_regs(cs,
                    A6XX_VSC_PRIM_STRM_PITCH(cmd->vsc_prim_strm_pitch),
@@ -1677,8 +1685,9 @@ static void
 emit_vsc_overflow_test(struct tu_cmd_buffer *cmd, struct tu_cs *cs)
 {
    const struct tu_tiling_config *tiling = cmd->state.tiling;
+   const struct tu_vsc_config *vsc = &tiling->vsc;
    const uint32_t used_pipe_count =
-      tiling->pipe_count.width * tiling->pipe_count.height;
+      vsc->pipe_count.width * vsc->pipe_count.height;
 
    for (int i = 0; i < used_pipe_count; i++) {
       tu_cs_emit_pkt7(cs, CP_COND_WRITE5, 8);
@@ -2163,6 +2172,7 @@ tu6_tile_render_begin(struct tu_cmd_buffer *cmd, struct tu_cs *cs,
 {
    struct tu_physical_device *phys_dev = cmd->device->physical_device;
    const struct tu_tiling_config *tiling = cmd->state.tiling;
+   const struct tu_vsc_config *vsc = &tiling->vsc;
    tu_lrz_tiling_begin<CHIP>(cmd, cs);
 
    tu_cs_emit_pkt7(cs, CP_SKIP_IB2_ENABLE_GLOBAL, 1);
@@ -2221,18 +2231,18 @@ tu6_tile_render_begin(struct tu_cmd_buffer *cmd, struct tu_cs *cs,
       tu_cs_emit_pkt7(cs, CP_SKIP_IB2_ENABLE_LOCAL, 1);
       tu_cs_emit(cs, 0x1);
    } else {
-      if (tiling->binning_possible) {
+      if (vsc->binning_possible) {
          /* Mark all tiles as visible for tu6_emit_cond_for_load_stores(), since
           * the actual binner didn't run.
           */
-         int pipe_count = tiling->pipe_count.width * tiling->pipe_count.height;
+         int pipe_count = vsc->pipe_count.width * vsc->pipe_count.height;
          tu_cs_emit_pkt4(cs, REG_A6XX_VSC_STATE_REG(0), pipe_count);
          for (int i = 0; i < pipe_count; i++)
             tu_cs_emit(cs, ~0);
       }
    }
 
-   if (tiling->binning_possible) {
+   if (vsc->binning_possible) {
       /* Upload state regs to memory to be restored on skipsaverestore
        * preemption.
        */
@@ -2541,6 +2551,7 @@ tu_cmd_render_tiles(struct tu_cmd_buffer *cmd,
                     struct tu_renderpass_result *autotune_result)
 {
    const struct tu_tiling_config *tiling = cmd->state.tiling;
+   const struct tu_vsc_config *vsc = &tiling->vsc;
    const struct tu_image_view *fdm = NULL;
 
    if (cmd->state.pass->fragment_density_map.attachment != VK_ATTACHMENT_UNUSED) {
@@ -2567,19 +2578,19 @@ tu_cmd_render_tiles(struct tu_cmd_buffer *cmd,
    /* Note: we reverse the order of walking the pipes and tiles on every
     * other row, to improve texture cache locality compared to raster order.
     */
-   for (uint32_t py = 0; py < tiling->pipe_count.height; py++) {
-      uint32_t pipe_row = py * tiling->pipe_count.width;
-      for (uint32_t pipe_row_i = 0; pipe_row_i < tiling->pipe_count.width; pipe_row_i++) {
+   for (uint32_t py = 0; py < vsc->pipe_count.height; py++) {
+      uint32_t pipe_row = py * vsc->pipe_count.width;
+      for (uint32_t pipe_row_i = 0; pipe_row_i < vsc->pipe_count.width; pipe_row_i++) {
          uint32_t px;
          if (py & 1)
-            px = tiling->pipe_count.width - 1 - pipe_row_i;
+            px = vsc->pipe_count.width - 1 - pipe_row_i;
          else
             px = pipe_row_i;
          uint32_t pipe = pipe_row + px;
-         uint32_t tx1 = px * tiling->pipe0.width;
-         uint32_t ty1 = py * tiling->pipe0.height;
-         uint32_t tx2 = MIN2(tx1 + tiling->pipe0.width, tiling->tile_count.width);
-         uint32_t ty2 = MIN2(ty1 + tiling->pipe0.height, tiling->tile_count.height);
+         uint32_t tx1 = px * vsc->pipe0.width;
+         uint32_t ty1 = py * vsc->pipe0.height;
+         uint32_t tx2 = MIN2(tx1 + vsc->pipe0.width, vsc->tile_count.width);
+         uint32_t ty2 = MIN2(ty1 + vsc->pipe0.height, vsc->tile_count.height);
 
          if (merge_tiles) {
             tu_render_pipe_fdm<CHIP>(cmd, pipe, tx1, ty1, tx2, ty2, fdm);
@@ -4836,6 +4847,7 @@ tu_emit_subpass_begin_gmem(struct tu_cmd_buffer *cmd, struct tu_resolve_group *r
 {
    struct tu_cs *cs = &cmd->draw_cs;
    uint32_t subpass_idx = cmd->state.subpass - cmd->state.pass->subpasses;
+   const struct tu_vsc_config *vsc = &cmd->state.tiling->vsc;
 
    /* If we might choose to bin, then put the loads under a check for geometry
     * having been binned to this tile.  If we don't choose to bin in the end,
@@ -4845,7 +4857,7 @@ tu_emit_subpass_begin_gmem(struct tu_cmd_buffer *cmd, struct tu_resolve_group *r
     * (perf queries), then we can't do this optimization since the
     * start-of-the-CS geometry condition will have been overwritten.
     */
-   bool cond_load_allowed = cmd->state.tiling->binning &&
+   bool cond_load_allowed = vsc->binning &&
                             cmd->state.pass->has_cond_load_store &&
                             !cmd->state.rp.draw_cs_writes_to_cond_pred;
 
diff --git a/src/freedreno/vulkan/tu_device.h b/src/freedreno/vulkan/tu_device.h
index e5d0057c05c80..e8277967a0bc5 100644
--- a/src/freedreno/vulkan/tu_device.h
+++ b/src/freedreno/vulkan/tu_device.h
@@ -462,31 +462,35 @@ struct tu_attachment_info
    struct tu_image_view *attachment;
 };
 
-struct tu_tiling_config {
-   /* size of the first tile */
-   VkExtent2D tile0;
+struct tu_vsc_config {
    /* number of tiles */
    VkExtent2D tile_count;
-
    /* size of the first VSC pipe */
    VkExtent2D pipe0;
    /* number of VSC pipes */
    VkExtent2D pipe_count;
 
-   /* Whether using GMEM is even possible with this configuration */
-   bool possible;
+   /* Whether binning could be used for gmem rendering using this framebuffer. */
+   bool binning_possible;
 
    /* Whether binning should be used for gmem rendering using this framebuffer. */
    bool binning;
 
-   /* Whether binning could be used for gmem rendering using this framebuffer. */
-   bool binning_possible;
-
    /* pipe register values */
    uint32_t pipe_config[MAX_VSC_PIPES];
    uint32_t pipe_sizes[MAX_VSC_PIPES];
 };
 
+struct tu_tiling_config {
+   /* size of the first tile */
+   VkExtent2D tile0;
+
+   /* Whether using GMEM is even possible with this configuration */
+   bool possible;
+
+   struct tu_vsc_config vsc;
+};
+
 struct tu_framebuffer
 {
    struct vk_object_base base;
diff --git a/src/freedreno/vulkan/tu_tracepoints.py b/src/freedreno/vulkan/tu_tracepoints.py
index 287e251d93129..a6391da404b2a 100644
--- a/src/freedreno/vulkan/tu_tracepoints.py
+++ b/src/freedreno/vulkan/tu_tracepoints.py
@@ -89,7 +89,7 @@ begin_end_tp('render_pass',
     tp_struct=[Arg(type='uint16_t', name='width',               var='fb->width',                                            c_format='%u'),
                Arg(type='uint16_t', name='height',              var='fb->height',                                           c_format='%u'),
                Arg(type='uint8_t',  name='attachment_count',    var='fb->attachment_count',                                 c_format='%u'),
-               Arg(type='uint16_t', name='numberOfBins',        var='tiling->tile_count.width * tiling->tile_count.height', c_format='%u'),
+               Arg(type='uint16_t', name='numberOfBins',        var='tiling->vsc.tile_count.width * tiling->vsc.tile_count.height', c_format='%u'),
                Arg(type='uint16_t', name='binWidth',            var='tiling->tile0.width',                                  c_format='%u'),
                Arg(type='uint16_t', name='binHeight',           var='tiling->tile0.height',                                 c_format='%u'),],
     # Args known only at the end of the renderpass:
diff --git a/src/freedreno/vulkan/tu_util.cc b/src/freedreno/vulkan/tu_util.cc
index 7feae56552aa2..07e13dd88811e 100644
--- a/src/freedreno/vulkan/tu_util.cc
+++ b/src/freedreno/vulkan/tu_util.cc
@@ -220,8 +220,10 @@ tu_tiling_config_update_tile_layout(struct tu_framebuffer *fb,
        * them, since you shouldn't be doing gmem work if gmem is not possible.
        */
       .tile0 = (VkExtent2D) { ~0, ~0 },
-      .tile_count = (VkExtent2D) { .width = 1, .height = 1 },
       .possible = false,
+      .vsc = {
+         .tile_count = (VkExtent2D) { .width = 1, .height = 1 },
+      },
    };
 
    /* From the Vulkan 1.3.232 spec, under VkFramebufferCreateInfo:
@@ -300,26 +302,26 @@ tu_tiling_config_update_tile_layout(struct tu_framebuffer *fb,
               abs((int)(tiling->tile0.width - tiling->tile0.height)))) {
          tiling->possible = true;
          tiling->tile0 = tile_size;
-         tiling->tile_count = tile_count;
+         tiling->vsc.tile_count = tile_count;
          best_tile_count = tile_count.width * tile_count.height;
       }
    }
 
    /* If forcing binning, try to get at least 2 tiles in each direction. */
    if (TU_DEBUG(FORCEBIN) && tiling->possible) {
-      if (tiling->tile_count.width == 1 && tiling->tile0.width != tile_align_w) {
+      if (tiling->vsc.tile_count.width == 1 && tiling->tile0.width != tile_align_w) {
          tiling->tile0.width = util_align_npot(DIV_ROUND_UP(tiling->tile0.width, 2), tile_align_w);
-         tiling->tile_count.width = 2;
+         tiling->vsc.tile_count.width = 2;
       }
-      if (tiling->tile_count.height == 1 && tiling->tile0.height != tile_align_h) {
+      if (tiling->vsc.tile_count.height == 1 && tiling->tile0.height != tile_align_h) {
          tiling->tile0.height = align(DIV_ROUND_UP(tiling->tile0.height, 2), tile_align_h);
-         tiling->tile_count.height = 2;
+         tiling->vsc.tile_count.height = 2;
       }
    }
 }
 
 static void
-tu_tiling_config_update_pipe_layout(struct tu_tiling_config *tiling,
+tu_tiling_config_update_pipe_layout(struct tu_vsc_config *vsc,
                                     const struct tu_device *dev,
                                     bool fdm)
 {
@@ -334,100 +336,100 @@ tu_tiling_config_update_pipe_layout(struct tu_tiling_config *tiling,
     */
    if (fdm && dev->physical_device->info->a6xx.has_bin_mask &&
        !TU_DEBUG(NO_BIN_MERGING)) {
-      tiling->pipe0.width = 4;
-      tiling->pipe0.height = 8;
-      tiling->pipe_count.width =
-         DIV_ROUND_UP(tiling->tile_count.width, tiling->pipe0.width);
-      tiling->pipe_count.height =
-         DIV_ROUND_UP(tiling->tile_count.height, tiling->pipe0.height);
+      vsc->pipe0.width = 4;
+      vsc->pipe0.height = 8;
+      vsc->pipe_count.width =
+         DIV_ROUND_UP(vsc->tile_count.width, vsc->pipe0.width);
+      vsc->pipe_count.height =
+         DIV_ROUND_UP(vsc->tile_count.height, vsc->pipe0.height);
       return;
    }
 
    /* start from 1 tile per pipe */
-   tiling->pipe0 = (VkExtent2D) {
+   vsc->pipe0 = (VkExtent2D) {
       .width = 1,
       .height = 1,
    };
-   tiling->pipe_count = tiling->tile_count;
+   vsc->pipe_count = vsc->tile_count;
 
-   while (tiling->pipe_count.width * tiling->pipe_count.height > max_pipe_count) {
-      if (tiling->pipe0.width < tiling->pipe0.height) {
-         tiling->pipe0.width += 1;
-         tiling->pipe_count.width =
-            DIV_ROUND_UP(tiling->tile_count.width, tiling->pipe0.width);
+   while (vsc->pipe_count.width * vsc->pipe_count.height > max_pipe_count) {
+      if (vsc->pipe0.width < vsc->pipe0.height) {
+         vsc->pipe0.width += 1;
+         vsc->pipe_count.width =
+            DIV_ROUND_UP(vsc->tile_count.width, vsc->pipe0.width);
       } else {
-         tiling->pipe0.height += 1;
-         tiling->pipe_count.height =
-            DIV_ROUND_UP(tiling->tile_count.height, tiling->pipe0.height);
+         vsc->pipe0.height += 1;
+         vsc->pipe_count.height =
+            DIV_ROUND_UP(vsc->tile_count.height, vsc->pipe0.height);
       }
    }
 }
 
 static void
-tu_tiling_config_update_pipes(struct tu_tiling_config *tiling,
+tu_tiling_config_update_pipes(struct tu_vsc_config *vsc,
                               const struct tu_device *dev)
 {
    const uint32_t max_pipe_count =
       dev->physical_device->info->num_vsc_pipes;
    const uint32_t used_pipe_count =
-      tiling->pipe_count.width * tiling->pipe_count.height;
+      vsc->pipe_count.width * vsc->pipe_count.height;
    const VkExtent2D last_pipe = {
-      .width = (tiling->tile_count.width - 1) % tiling->pipe0.width + 1,
-      .height = (tiling->tile_count.height - 1) % tiling->pipe0.height + 1,
+      .width = (vsc->tile_count.width - 1) % vsc->pipe0.width + 1,
+      .height = (vsc->tile_count.height - 1) % vsc->pipe0.height + 1,
    };
 
    assert(used_pipe_count <= max_pipe_count);
-   assert(max_pipe_count <= ARRAY_SIZE(tiling->pipe_config));
+   assert(max_pipe_count <= ARRAY_SIZE(vsc->pipe_config));
 
-   for (uint32_t y = 0; y < tiling->pipe_count.height; y++) {
-      for (uint32_t x = 0; x < tiling->pipe_count.width; x++) {
-         const uint32_t pipe_x = tiling->pipe0.width * x;
-         const uint32_t pipe_y = tiling->pipe0.height * y;
-         const uint32_t pipe_w = (x == tiling->pipe_count.width - 1)
+   for (uint32_t y = 0; y < vsc->pipe_count.height; y++) {
+      for (uint32_t x = 0; x < vsc->pipe_count.width; x++) {
+         const uint32_t pipe_x = vsc->pipe0.width * x;
+         const uint32_t pipe_y = vsc->pipe0.height * y;
+         const uint32_t pipe_w = (x == vsc->pipe_count.width - 1)
                                     ? last_pipe.width
-                                    : tiling->pipe0.width;
-         const uint32_t pipe_h = (y == tiling->pipe_count.height - 1)
+                                    : vsc->pipe0.width;
+         const uint32_t pipe_h = (y == vsc->pipe_count.height - 1)
                                     ? last_pipe.height
-                                    : tiling->pipe0.height;
-         const uint32_t n = tiling->pipe_count.width * y + x;
+                                    : vsc->pipe0.height;
+         const uint32_t n = vsc->pipe_count.width * y + x;
 
-         tiling->pipe_config[n] = A6XX_VSC_PIPE_CONFIG_REG_X(pipe_x) |
+         vsc->pipe_config[n] = A6XX_VSC_PIPE_CONFIG_REG_X(pipe_x) |
                                   A6XX_VSC_PIPE_CONFIG_REG_Y(pipe_y) |
                                   A6XX_VSC_PIPE_CONFIG_REG_W(pipe_w) |
                                   A6XX_VSC_PIPE_CONFIG_REG_H(pipe_h);
-         tiling->pipe_sizes[n] = CP_SET_BIN_DATA5_0_VSC_SIZE(pipe_w * pipe_h);
+         vsc->pipe_sizes[n] = CP_SET_BIN_DATA5_0_VSC_SIZE(pipe_w * pipe_h);
       }
    }
 
-   memset(tiling->pipe_config + used_pipe_count, 0,
+   memset(vsc->pipe_config + used_pipe_count, 0,
           sizeof(uint32_t) * (max_pipe_count - used_pipe_count));
 }
 
 static bool
-is_hw_binning_possible(const struct tu_tiling_config *tiling)
+is_hw_binning_possible(const struct tu_vsc_config *vsc)
 {
    /* Similar to older gens, # of tiles per pipe cannot be more than 32.
     * But there are no hangs with 16 or more tiles per pipe in either
     * X or Y direction, so that limit does not seem to apply.
     */
-   uint32_t tiles_per_pipe = tiling->pipe0.width * tiling->pipe0.height;
+   uint32_t tiles_per_pipe = vsc->pipe0.width * vsc->pipe0.height;
    return tiles_per_pipe <= 32;
 }
 
 static void
-tu_tiling_config_update_binning(struct tu_tiling_config *tiling, const struct tu_device *device)
+tu_tiling_config_update_binning(struct tu_vsc_config *vsc, const struct tu_device *device)
 {
-   tiling->binning_possible = is_hw_binning_possible(tiling);
+   vsc->binning_possible = is_hw_binning_possible(vsc);
 
-   if (tiling->binning_possible) {
-      tiling->binning = (tiling->tile_count.width * tiling->tile_count.height) > 2;
+   if (vsc->binning_possible) {
+      vsc->binning = (vsc->tile_count.width * vsc->tile_count.height) > 2;
 
       if (TU_DEBUG(FORCEBIN))
-         tiling->binning = true;
+         vsc->binning = true;
       if (TU_DEBUG(NOBIN))
-         tiling->binning = false;
+         vsc->binning = false;
    } else {
-      tiling->binning = false;
+      vsc->binning = false;
    }
 }
 
@@ -443,9 +445,10 @@ tu_framebuffer_tiling_config(struct tu_framebuffer *fb,
       if (!tiling->possible)
          continue;
 
-      tu_tiling_config_update_pipe_layout(tiling, device, pass->has_fdm);
-      tu_tiling_config_update_pipes(tiling, device);
-      tu_tiling_config_update_binning(tiling, device);
+      struct tu_vsc_config *vsc = &tiling->vsc;
+      tu_tiling_config_update_pipe_layout(vsc, device, pass->has_fdm);
+      tu_tiling_config_update_pipes(vsc, device);
+      tu_tiling_config_update_binning(vsc, device);
    }
 }
 
-- 
GitLab


From 16da4f8731ec2f6075ca9637f04e3b3da6244014 Mon Sep 17 00:00:00 2001
From: Connor Abbott <cwabbott0@gmail.com>
Date: Thu, 30 Jan 2025 13:48:44 -0500
Subject: [PATCH 05/12] tu: Fix CmdClearAttachments with fragment density map

The clear may be a partial clear, in which case we need to make sure
that the clear rectangle is transformed into GMEM space so that it is
clipped correctly.
---
 src/freedreno/vulkan/tu_clear_blit.cc | 204 ++++++++++++++++++++++----
 src/freedreno/vulkan/tu_cmd_buffer.cc |  10 ++
 src/freedreno/vulkan/tu_cmd_buffer.h  |   2 +
 3 files changed, 190 insertions(+), 26 deletions(-)

diff --git a/src/freedreno/vulkan/tu_clear_blit.cc b/src/freedreno/vulkan/tu_clear_blit.cc
index 7d15d67536ee6..f2f58ecd4ed30 100644
--- a/src/freedreno/vulkan/tu_clear_blit.cc
+++ b/src/freedreno/vulkan/tu_clear_blit.cc
@@ -3899,6 +3899,48 @@ remap_attachment(struct tu_cmd_buffer *cmd, unsigned a)
    return i;
 }
 
+struct apply_sysmem_clear_coords_state {
+   unsigned view;
+   unsigned layer;
+   float z_clear_val;
+   VkRect2D rect;
+};
+
+static void
+fdm_apply_sysmem_clear_coords(struct tu_cmd_buffer *cmd,
+                              struct tu_cs *cs,
+                              void *data,
+                              VkRect2D bin,
+                              unsigned views,
+                              const VkExtent2D *frag_areas)
+{
+   const struct apply_sysmem_clear_coords_state *state =
+      (const struct apply_sysmem_clear_coords_state *)data;
+   assert(state->view < views);
+
+   VkExtent2D frag_area = frag_areas[state->view];
+
+   VkOffset2D offset = tu_fdm_per_bin_offset(frag_area, bin);
+
+   unsigned x1 = state->rect.offset.x / frag_area.width + offset.x;
+   unsigned x2 = DIV_ROUND_UP(state->rect.offset.x + state->rect.extent.width,
+                              frag_area.width) + offset.x;
+   unsigned y1 = state->rect.offset.y / frag_area.height + offset.y;
+   unsigned y2 = DIV_ROUND_UP(state->rect.offset.y + state->rect.extent.height,
+                              frag_area.height) + offset.y;
+
+   const float coords[] = {
+      x1, y1,
+      state->z_clear_val,
+      uif(state->layer),
+      x2, y2,
+      state->z_clear_val,
+      1.0f,
+   };
+
+   r3d_coords_raw(cmd, cs, coords);
+}
+
 template <chip CHIP>
 static void
 tu_clear_sysmem_attachments(struct tu_cmd_buffer *cmd,
@@ -4036,6 +4078,9 @@ tu_clear_sysmem_attachments(struct tu_cmd_buffer *cmd,
       tu6_emit_blit_consts_load(cmd, cs, CP_LOAD_STATE6_FRAG, SB6_FS_SHADER,
                                 0, packed_clear_value, num_rts);
 
+   if (cmd->state.fdm_enabled)
+      tu_cs_set_writeable(cs, true);
+
    for (uint32_t i = 0; i < rect_count; i++) {
       /* This should be true because of this valid usage for
        * vkCmdClearAttachments:
@@ -4053,18 +4098,31 @@ tu_clear_sysmem_attachments(struct tu_cmd_buffer *cmd,
        */
       for_each_layer(layer, subpass->multiview_mask, rects[i].layerCount)
       {
-         const float coords[] = {
-            rects[i].rect.offset.x,
-            rects[i].rect.offset.y,
-            z_clear_val,
-            uif(rects[i].baseArrayLayer + layer),
-            rects[i].rect.offset.x + rects[i].rect.extent.width,
-            rects[i].rect.offset.y + rects[i].rect.extent.height,
-            z_clear_val,
-            1.0f,
-         };
+         if (cmd->state.fdm_enabled) {
+            struct apply_sysmem_clear_coords_state state = {
+               .view = subpass->multiview_mask ? layer : 0,
+               .layer = rects[i].baseArrayLayer + layer,
+               .z_clear_val = z_clear_val,
+               .rect = rects[i].rect,
+            };
+            tu_create_fdm_bin_patchpoint(cmd, cs, 4,
+                                         fdm_apply_sysmem_clear_coords,
+                                         state);
+         } else {
+            const float coords[] = {
+               rects[i].rect.offset.x,
+               rects[i].rect.offset.y,
+               z_clear_val,
+               uif(rects[i].baseArrayLayer + layer),
+               rects[i].rect.offset.x + rects[i].rect.extent.width,
+               rects[i].rect.offset.y + rects[i].rect.extent.height,
+               z_clear_val,
+               1.0f,
+            };
+
+            r3d_coords_raw(cmd, cs, coords);
+         }
 
-         r3d_coords_raw(cmd, cs, coords);
          r3d_run_vis(cmd, cs);
       }
    }
@@ -4076,6 +4134,9 @@ tu_clear_sysmem_attachments(struct tu_cmd_buffer *cmd,
       tu_emit_event_write<CHIP>(cmd, cs, FD_START_PRIMITIVE_CTRS);
    }
 
+   if (cmd->state.fdm_enabled)
+      tu_cs_set_writeable(cs, false);
+
    trace_end_sysmem_clear_all(&cmd->trace, cs);
 }
 
@@ -4112,6 +4173,41 @@ clear_gmem_attachment(struct tu_cmd_buffer *cmd,
    tu_emit_event_write<CHIP>(cmd, cs, FD_BLIT);
 }
 
+struct apply_gmem_clear_coords_state {
+   unsigned view;
+   VkRect2D rect;
+};
+
+static void
+fdm_apply_gmem_clear_coords(struct tu_cmd_buffer *cmd,
+                            struct tu_cs *cs,
+                            void *data,
+                            VkRect2D bin,
+                            unsigned views,
+                            const VkExtent2D *frag_areas)
+{
+   const struct apply_gmem_clear_coords_state *state =
+      (const struct apply_gmem_clear_coords_state *)data;
+   assert(state->view < views);
+
+   VkExtent2D frag_area = frag_areas[state->view];
+
+   VkOffset2D offset = tu_fdm_per_bin_offset(frag_area, bin);
+
+   unsigned x1 = state->rect.offset.x / frag_area.width + offset.x;
+   unsigned x2 = DIV_ROUND_UP(state->rect.offset.x + state->rect.extent.width,
+                              frag_area.width) + offset.x - 1;
+   unsigned y1 = state->rect.offset.y / frag_area.height + offset.y;
+   unsigned y2 = DIV_ROUND_UP(state->rect.offset.y + state->rect.extent.height,
+                              frag_area.height) + offset.y - 1;
+
+   tu_cs_emit_pkt4(cs, REG_A6XX_RB_BLIT_SCISSOR_TL, 2);
+   tu_cs_emit(cs,
+              A6XX_RB_BLIT_SCISSOR_TL_X(x1) | A6XX_RB_BLIT_SCISSOR_TL_Y(y1));
+   tu_cs_emit(cs,
+              A6XX_RB_BLIT_SCISSOR_BR_X(x2) | A6XX_RB_BLIT_SCISSOR_BR_Y(y2));
+}
+
 template <chip CHIP>
 static void
 tu_emit_clear_gmem_attachment(struct tu_cmd_buffer *cmd,
@@ -4122,7 +4218,8 @@ tu_emit_clear_gmem_attachment(struct tu_cmd_buffer *cmd,
                               uint32_t layers,
                               uint32_t layer_mask,
                               VkImageAspectFlags mask,
-                              const VkClearValue *value)
+                              const VkClearValue *value,
+                              const VkRect2D *fdm_rect)
 {
    const struct tu_render_pass_attachment *att =
       &cmd->state.pass->attachments[attachment];
@@ -4135,6 +4232,14 @@ tu_emit_clear_gmem_attachment(struct tu_cmd_buffer *cmd,
    enum pipe_format format = vk_format_to_pipe_format(att->format);
    for_each_layer(i, layer_mask, layers) {
       uint32_t layer = i + base_layer;
+      if (fdm_rect) {
+            struct apply_gmem_clear_coords_state state = {
+               .view = layer,
+               .rect = *fdm_rect,
+            };
+            tu_create_fdm_bin_patchpoint(cmd, cs, 3,
+                                         fdm_apply_gmem_clear_coords, state);
+      }
       if (att->format == VK_FORMAT_D32_SFLOAT_S8_UINT) {
          if (mask & VK_IMAGE_ASPECT_DEPTH_BIT) {
             uint32_t buffer_id = tu_resolve_group_include_buffer<CHIP>(resolve_group, VK_FORMAT_D32_SFLOAT);
@@ -4174,15 +4279,33 @@ tu_clear_gmem_attachments(struct tu_cmd_buffer *cmd,
 
    struct tu_resolve_group resolve_group = {};
 
+   if (cmd->state.fdm_enabled)
+      tu_cs_set_writeable(cs, true);
+
    for (unsigned i = 0; i < rect_count; i++) {
       unsigned x1 = rects[i].rect.offset.x;
       unsigned y1 = rects[i].rect.offset.y;
       unsigned x2 = x1 + rects[i].rect.extent.width - 1;
       unsigned y2 = y1 + rects[i].rect.extent.height - 1;
 
-      tu_cs_emit_pkt4(cs, REG_A6XX_RB_BLIT_SCISSOR_TL, 2);
-      tu_cs_emit(cs, A6XX_RB_BLIT_SCISSOR_TL_X(x1) | A6XX_RB_BLIT_SCISSOR_TL_Y(y1));
-      tu_cs_emit(cs, A6XX_RB_BLIT_SCISSOR_BR_X(x2) | A6XX_RB_BLIT_SCISSOR_BR_Y(y2));
+      const VkRect2D *fdm_rect = NULL;
+      if (cmd->state.fdm_enabled) {
+         if (!subpass->multiview_mask) {
+            struct apply_gmem_clear_coords_state state = {
+               .view = 0,
+               .rect = rects[i].rect,
+            };
+            tu_create_fdm_bin_patchpoint(cmd, cs, 3,
+                                         fdm_apply_gmem_clear_coords, state);
+         } else {
+            /* We need to patch the clear rectangle for each view. */
+            fdm_rect = &rects[i].rect;
+         }
+      } else {
+         tu_cs_emit_pkt4(cs, REG_A6XX_RB_BLIT_SCISSOR_TL, 2);
+         tu_cs_emit(cs, A6XX_RB_BLIT_SCISSOR_TL_X(x1) | A6XX_RB_BLIT_SCISSOR_TL_Y(y1));
+         tu_cs_emit(cs, A6XX_RB_BLIT_SCISSOR_BR_X(x2) | A6XX_RB_BLIT_SCISSOR_BR_Y(y2));
+      }
 
       for (unsigned j = 0; j < attachment_count; j++) {
          uint32_t a;
@@ -4199,11 +4322,15 @@ tu_clear_gmem_attachments(struct tu_cmd_buffer *cmd,
                                        rects[i].layerCount,
                                        subpass->multiview_mask,
                                        attachments[j].aspectMask,
-                                       &attachments[j].clearValue);
+                                       &attachments[j].clearValue,
+                                       fdm_rect);
       }
    }
 
    tu_emit_resolve_group<CHIP>(cmd, cs, &resolve_group);
+
+   if (cmd->state.fdm_enabled)
+      tu_cs_set_writeable(cs, false);
 }
 
 template <chip CHIP>
@@ -4277,16 +4404,29 @@ tu7_clear_attachment_generic_single_rect(
    const VkClearRect *rect)
 {
    const struct tu_subpass *subpass = cmd->state.subpass;
-   unsigned x1 = rect->rect.offset.x;
-   unsigned y1 = rect->rect.offset.y;
-   unsigned x2 = x1 + rect->rect.extent.width - 1;
-   unsigned y2 = y1 + rect->rect.extent.height - 1;
 
-   tu_cs_emit_pkt4(cs, REG_A6XX_RB_BLIT_SCISSOR_TL, 2);
-   tu_cs_emit(cs,
-              A6XX_RB_BLIT_SCISSOR_TL_X(x1) | A6XX_RB_BLIT_SCISSOR_TL_Y(y1));
-   tu_cs_emit(cs,
-              A6XX_RB_BLIT_SCISSOR_BR_X(x2) | A6XX_RB_BLIT_SCISSOR_BR_Y(y2));
+   if (cmd->state.fdm_enabled) {
+      tu_cs_set_writeable(cs, true);
+
+      if (!subpass->multiview_mask) {
+            struct apply_gmem_clear_coords_state state = {
+               .view = 0,
+               .rect = rect->rect,
+            };
+            tu_create_fdm_bin_patchpoint(cmd, cs, 3,
+                                         fdm_apply_gmem_clear_coords, state);
+      }
+   } else {
+      unsigned x1 = rect->rect.offset.x;
+      unsigned y1 = rect->rect.offset.y;
+      unsigned x2 = x1 + rect->rect.extent.width - 1;
+      unsigned y2 = y1 + rect->rect.extent.height - 1;
+      tu_cs_emit_pkt4(cs, REG_A6XX_RB_BLIT_SCISSOR_TL, 2);
+      tu_cs_emit(cs,
+                 A6XX_RB_BLIT_SCISSOR_TL_X(x1) | A6XX_RB_BLIT_SCISSOR_TL_Y(y1));
+      tu_cs_emit(cs,
+                 A6XX_RB_BLIT_SCISSOR_BR_X(x2) | A6XX_RB_BLIT_SCISSOR_BR_Y(y2));
+   }
 
    auto value = &clear_att->clearValue;
 
@@ -4296,6 +4436,15 @@ tu7_clear_attachment_generic_single_rect(
       uint32_t mask =
          aspect_write_mask_generic_clear(format, clear_att->aspectMask);
 
+      if (cmd->state.fdm_enabled && subpass->multiview_mask) {
+            struct apply_gmem_clear_coords_state state = {
+               .view = layer,
+               .rect = rect->rect,
+            };
+            tu_create_fdm_bin_patchpoint(cmd, cs, 3,
+                                         fdm_apply_gmem_clear_coords, state);
+      }
+
       if (att->format == VK_FORMAT_D32_SFLOAT_S8_UINT) {
          if (clear_att->aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT) {
             uint32_t buffer_id = tu_resolve_group_include_buffer<A7XX>(resolve_group, VK_FORMAT_D32_SFLOAT);
@@ -4312,6 +4461,9 @@ tu7_clear_attachment_generic_single_rect(
          tu7_generic_layer_clear(cmd, cs, buffer_id, format, mask, false, layer, value, a);
       }
    }
+
+   if (cmd->state.fdm_enabled)
+      tu_cs_set_writeable(cs, false);
 }
 
 static void
@@ -4512,7 +4664,7 @@ tu_clear_gmem_attachment(struct tu_cmd_buffer *cmd,
                                  cmd->state.framebuffer->layers,
                                  attachment->clear_views,
                                  attachment->clear_mask,
-                                 &cmd->state.clear_values[a]);
+                                 &cmd->state.clear_values[a], NULL);
 }
 TU_GENX(tu_clear_gmem_attachment);
 
diff --git a/src/freedreno/vulkan/tu_cmd_buffer.cc b/src/freedreno/vulkan/tu_cmd_buffer.cc
index 669f72351de5d..59084db1be14c 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.cc
+++ b/src/freedreno/vulkan/tu_cmd_buffer.cc
@@ -2083,6 +2083,8 @@ tu_emit_renderpass_begin(struct tu_cmd_buffer *cmd)
     */
    BITSET_SET(cmd->vk.dynamic_graphics_state.dirty,
               MESA_VK_DYNAMIC_IA_PRIMITIVE_RESTART_ENABLE);
+
+   cmd->state.fdm_enabled = cmd->state.pass->has_fdm;
 }
 
 template <chip CHIP>
@@ -2962,10 +2964,18 @@ tu_BeginCommandBuffer(VkCommandBuffer commandBuffer,
                vk_common_CmdSetRenderingAttachmentLocationsKHR(commandBuffer,
                                                                location_info);
             }
+            /* Unfortunately with dynamic renderpasses we get no indication
+             * whether FDM is used in secondaries, so we have to assume it
+             * always might be enabled.
+             */
+            cmd_buffer->state.fdm_enabled = 
+               cmd_buffer->device->vk.enabled_features.fragmentDensityMap ||
+               TU_DEBUG(FDM);
          } else {
             cmd_buffer->state.pass = tu_render_pass_from_handle(pBeginInfo->pInheritanceInfo->renderPass);
             cmd_buffer->state.subpass =
                &cmd_buffer->state.pass->subpasses[pBeginInfo->pInheritanceInfo->subpass];
+            cmd_buffer->state.fdm_enabled = cmd_buffer->state.pass->has_fdm;
          }
          tu_fill_render_pass_state(&cmd_buffer->state.vk_rp,
                                    cmd_buffer->state.pass,
diff --git a/src/freedreno/vulkan/tu_cmd_buffer.h b/src/freedreno/vulkan/tu_cmd_buffer.h
index 958166eb36ac9..707cda0a09bfc 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.h
+++ b/src/freedreno/vulkan/tu_cmd_buffer.h
@@ -510,6 +510,8 @@ struct tu_cmd_state
       struct tu_lrz_state lrz;
    } suspended_pass;
 
+   bool fdm_enabled;
+
    bool tessfactor_addr_set;
    bool predication_active;
    bool msaa_disable;
-- 
GitLab


From 2597c23788f1d3a0f4af85767b44e591bfdf1e2f Mon Sep 17 00:00:00 2001
From: Connor Abbott <cwabbott0@gmail.com>
Date: Tue, 11 Feb 2025 10:49:38 -0500
Subject: [PATCH 06/12] tu/fdm: Skip some patchpoints when binning

In order to implement FDM offset, we will have to offset the viewport
and scissor in the binning pass. In order to do this, we have to pass a
bin with nonsensical negative offsets to the patchpoint function, which
would result in asserts when patching the load/store sequences. But we
don't really need to patch these anyways as they are unused during
binning, so add the ability to skip them when binning. FS params and
some implementations of CmdClearAttachments (that don't contribute to
visibility) can similarly be skipped.
---
 src/freedreno/vulkan/tu_clear_blit.cc | 17 +++++++++--------
 src/freedreno/vulkan/tu_cmd_buffer.cc |  3 +++
 src/freedreno/vulkan/tu_cmd_buffer.h  | 14 ++++++++++++--
 src/freedreno/vulkan/tu_pipeline.cc   |  6 ++++--
 4 files changed, 28 insertions(+), 12 deletions(-)

diff --git a/src/freedreno/vulkan/tu_clear_blit.cc b/src/freedreno/vulkan/tu_clear_blit.cc
index f2f58ecd4ed30..0547370b60b81 100644
--- a/src/freedreno/vulkan/tu_clear_blit.cc
+++ b/src/freedreno/vulkan/tu_clear_blit.cc
@@ -4105,7 +4105,7 @@ tu_clear_sysmem_attachments(struct tu_cmd_buffer *cmd,
                .z_clear_val = z_clear_val,
                .rect = rects[i].rect,
             };
-            tu_create_fdm_bin_patchpoint(cmd, cs, 4,
+            tu_create_fdm_bin_patchpoint(cmd, cs, 4, TU_FDM_NONE,
                                          fdm_apply_sysmem_clear_coords,
                                          state);
          } else {
@@ -4237,7 +4237,7 @@ tu_emit_clear_gmem_attachment(struct tu_cmd_buffer *cmd,
                .view = layer,
                .rect = *fdm_rect,
             };
-            tu_create_fdm_bin_patchpoint(cmd, cs, 3,
+            tu_create_fdm_bin_patchpoint(cmd, cs, 3, TU_FDM_SKIP_BINNING,
                                          fdm_apply_gmem_clear_coords, state);
       }
       if (att->format == VK_FORMAT_D32_SFLOAT_S8_UINT) {
@@ -4295,7 +4295,7 @@ tu_clear_gmem_attachments(struct tu_cmd_buffer *cmd,
                .view = 0,
                .rect = rects[i].rect,
             };
-            tu_create_fdm_bin_patchpoint(cmd, cs, 3,
+            tu_create_fdm_bin_patchpoint(cmd, cs, 3, TU_FDM_SKIP_BINNING,
                                          fdm_apply_gmem_clear_coords, state);
          } else {
             /* We need to patch the clear rectangle for each view. */
@@ -4413,7 +4413,7 @@ tu7_clear_attachment_generic_single_rect(
                .view = 0,
                .rect = rect->rect,
             };
-            tu_create_fdm_bin_patchpoint(cmd, cs, 3,
+            tu_create_fdm_bin_patchpoint(cmd, cs, 3, TU_FDM_SKIP_BINNING,
                                          fdm_apply_gmem_clear_coords, state);
       }
    } else {
@@ -4441,7 +4441,7 @@ tu7_clear_attachment_generic_single_rect(
                .view = layer,
                .rect = rect->rect,
             };
-            tu_create_fdm_bin_patchpoint(cmd, cs, 3,
+            tu_create_fdm_bin_patchpoint(cmd, cs, 3, TU_FDM_SKIP_BINNING,
                                          fdm_apply_gmem_clear_coords, state);
       }
 
@@ -4877,7 +4877,8 @@ load_3d_blit(struct tu_cmd_buffer *cmd,
          struct apply_load_coords_state state = {
             .view = att->clear_views ? i : 0,
          };
-         tu_create_fdm_bin_patchpoint(cmd, cs, 4, fdm_apply_load_coords, state);
+         tu_create_fdm_bin_patchpoint(cmd, cs, 4, TU_FDM_SKIP_BINNING,
+                                      fdm_apply_load_coords, state);
       }
 
       r3d_dst_gmem<CHIP>(cmd, cs, iview, att, separate_stencil, i);
@@ -5436,8 +5437,8 @@ tu_store_gmem_attachment(struct tu_cmd_buffer *cmd,
             struct apply_store_coords_state state = {
                .view = view,
             };
-            tu_create_fdm_bin_patchpoint(cmd, cs, 8, fdm_apply_store_coords,
-                                         state);
+            tu_create_fdm_bin_patchpoint(cmd, cs, 8, TU_FDM_SKIP_BINNING,
+                                         fdm_apply_store_coords, state);
          }
          if (store_common) {
             store_cp_blit<CHIP>(cmd, cs, src_iview, dst_iview, src->samples, false, src_format,
diff --git a/src/freedreno/vulkan/tu_cmd_buffer.cc b/src/freedreno/vulkan/tu_cmd_buffer.cc
index 59084db1be14c..1f05a9d622821 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.cc
+++ b/src/freedreno/vulkan/tu_cmd_buffer.cc
@@ -1734,6 +1734,8 @@ tu6_emit_binning_pass(struct tu_cmd_buffer *cmd, struct tu_cs *cs)
       VkRect2D bin = { { 0, 0 }, { fb->width, fb->height } };
       util_dynarray_foreach (&cmd->fdm_bin_patchpoints,
                              struct tu_fdm_bin_patchpoint, patch) {
+         if (patch->flags & TU_FDM_SKIP_BINNING)
+            continue;
          tu_cs_emit_pkt7(cs, CP_MEM_WRITE, 2 + patch->size);
          tu_cs_emit_qw(cs, patch->iova);
          patch->apply(cmd, cs, patch->data, bin, num_views, unscaled_frag_areas);
@@ -5922,6 +5924,7 @@ tu_emit_fdm_params(struct tu_cmd_buffer *cmd,
             .num_consts = num_units - 1,
          };
          tu_create_fdm_bin_patchpoint(cmd, cs, 4 * (num_units - 1),
+                                      TU_FDM_SKIP_BINNING,
                                       fdm_apply_fs_params, state);
       } else {
          for (unsigned i = 1; i < num_units; i++) {
diff --git a/src/freedreno/vulkan/tu_cmd_buffer.h b/src/freedreno/vulkan/tu_cmd_buffer.h
index 707cda0a09bfc..3cc9f07e7db5b 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.h
+++ b/src/freedreno/vulkan/tu_cmd_buffer.h
@@ -751,9 +751,17 @@ typedef void (*tu_fdm_bin_apply_t)(struct tu_cmd_buffer *cmd,
                                    unsigned views,
                                    const VkExtent2D *frag_areas);
 
+enum tu_fdm_flags {
+   TU_FDM_NONE = 0,
+
+   /* Skip applying this patchpoint when binning */
+   TU_FDM_SKIP_BINNING = 1,
+};
+
 struct tu_fdm_bin_patchpoint {
    uint64_t iova;
    uint32_t size;
+   enum tu_fdm_flags flags;
    void *data;
    tu_fdm_bin_apply_t apply;
 };
@@ -773,6 +781,7 @@ static inline void
 _tu_create_fdm_bin_patchpoint(struct tu_cmd_buffer *cmd,
                               struct tu_cs *cs,
                               unsigned size,
+                              enum tu_fdm_flags flags,
                               tu_fdm_bin_apply_t apply,
                               void *state,
                               unsigned state_size)
@@ -784,6 +793,7 @@ _tu_create_fdm_bin_patchpoint(struct tu_cmd_buffer *cmd,
    struct tu_fdm_bin_patchpoint patch = {
       .iova = tu_cs_get_cur_iova(cs),
       .size = size,
+      .flags = flags,
       .data = data,
       .apply = apply,
    };
@@ -807,8 +817,8 @@ _tu_create_fdm_bin_patchpoint(struct tu_cmd_buffer *cmd,
                         patch);
 }
 
-#define tu_create_fdm_bin_patchpoint(cmd, cs, size, apply, state) \
-   _tu_create_fdm_bin_patchpoint(cmd, cs, size, apply, &state, sizeof(state))
+#define tu_create_fdm_bin_patchpoint(cmd, cs, size, flags, apply, state) \
+   _tu_create_fdm_bin_patchpoint(cmd, cs, size, flags, apply, &state, sizeof(state))
 
 VkResult tu_init_bin_preamble(struct tu_device *device);
 
diff --git a/src/freedreno/vulkan/tu_pipeline.cc b/src/freedreno/vulkan/tu_pipeline.cc
index c3a3e289f390a..7352886e767f2 100644
--- a/src/freedreno/vulkan/tu_pipeline.cc
+++ b/src/freedreno/vulkan/tu_pipeline.cc
@@ -2639,7 +2639,8 @@ tu6_emit_viewport_fdm(struct tu_cs *cs, struct tu_cmd_buffer *cmd,
       state.vp.viewport_count = num_views;
    unsigned size = TU_CALLX(cmd->device, tu6_viewport_size)(cmd->device, &state.vp, &state.rs);
    tu_cs_begin_sub_stream(&cmd->sub_cs, size, cs);
-   tu_create_fdm_bin_patchpoint(cmd, cs, size, fdm_apply_viewports, state);
+   tu_create_fdm_bin_patchpoint(cmd, cs, size, TU_FDM_NONE,
+                                fdm_apply_viewports, state);
    cmd->state.rp.shared_viewport |= !cmd->state.per_view_viewport;
 }
 
@@ -2751,7 +2752,8 @@ tu6_emit_scissor_fdm(struct tu_cs *cs, struct tu_cmd_buffer *cmd,
       state.vp.scissor_count = num_views;
    unsigned size = TU_CALLX(cmd->device, tu6_scissor_size)(cmd->device, &state.vp);
    tu_cs_begin_sub_stream(&cmd->sub_cs, size, cs);
-   tu_create_fdm_bin_patchpoint(cmd, cs, size, fdm_apply_scissors, state);
+   tu_create_fdm_bin_patchpoint(cmd, cs, size, TU_FDM_NONE, fdm_apply_scissors,
+                                state);
 }
 
 static const enum mesa_vk_dynamic_graphics_state tu_sample_locations_state[] = {
-- 
GitLab


From dd8869dd6e3a303b0eb7e99071c0b37559aa4d70 Mon Sep 17 00:00:00 2001
From: Connor Abbott <cwabbott0@gmail.com>
Date: Tue, 11 Feb 2025 12:40:59 -0500
Subject: [PATCH 07/12] tu: Implement VK_QCOM_fragment_density_map_offset

---
 src/freedreno/vulkan/tu_clear_blit.cc        |  77 +++-
 src/freedreno/vulkan/tu_cmd_buffer.cc        | 381 ++++++++++++++++---
 src/freedreno/vulkan/tu_cmd_buffer.h         |  17 +-
 src/freedreno/vulkan/tu_common.h             |  12 +
 src/freedreno/vulkan/tu_device.cc            |   9 +
 src/freedreno/vulkan/tu_device.h             |   2 +-
 src/freedreno/vulkan/tu_dynamic_rendering.cc |   2 +-
 src/freedreno/vulkan/tu_image.cc             |  29 +-
 src/freedreno/vulkan/tu_image.h              |   4 +-
 src/freedreno/vulkan/tu_lrz.cc               |   2 +-
 src/freedreno/vulkan/tu_pipeline.cc          |  64 ++--
 src/freedreno/vulkan/tu_pipeline.h           |   3 +-
 src/freedreno/vulkan/tu_util.cc              |  11 +
 src/freedreno/vulkan/tu_util.h               |   1 +
 14 files changed, 482 insertions(+), 132 deletions(-)

diff --git a/src/freedreno/vulkan/tu_clear_blit.cc b/src/freedreno/vulkan/tu_clear_blit.cc
index 0547370b60b81..8305bfd829293 100644
--- a/src/freedreno/vulkan/tu_clear_blit.cc
+++ b/src/freedreno/vulkan/tu_clear_blit.cc
@@ -1363,6 +1363,22 @@ r3d_src_gmem(struct tu_cmd_buffer *cmd,
    if (!iview->view.is_mutable)
       desc[0] &= ~A6XX_TEX_CONST_0_SWAP__MASK;
    desc[0] |= A6XX_TEX_CONST_0_TILE_MODE(TILE6_2);
+
+   /* If FDM offset is used, the last row and column extend beyond the
+    * framebuffer but are shifted over when storing. Expand the width and
+    * height to account for that.
+    */
+   if (tu_enable_fdm_offset(cmd)) {
+      uint32_t width = desc[1] & A6XX_TEX_CONST_1_WIDTH__MASK;
+      uint32_t height = (desc[1] & A6XX_TEX_CONST_1_HEIGHT__MASK) >>
+         A6XX_TEX_CONST_1_HEIGHT__SHIFT;
+      width += cmd->state.tiling->tile0.width;
+      height += cmd->state.tiling->tile0.height;
+      desc[1] = (desc[1] & ~(A6XX_TEX_CONST_1_WIDTH__MASK |
+                            A6XX_TEX_CONST_1_HEIGHT__MASK)) |
+         A6XX_TEX_CONST_1_WIDTH(width) | A6XX_TEX_CONST_1_HEIGHT(height);
+   }
+
    desc[2] =
       A6XX_TEX_CONST_2_TYPE(A6XX_TEX_2D) |
       A6XX_TEX_CONST_2_PITCH(cmd->state.tiling->tile0.width * cpp);
@@ -3910,17 +3926,19 @@ static void
 fdm_apply_sysmem_clear_coords(struct tu_cmd_buffer *cmd,
                               struct tu_cs *cs,
                               void *data,
-                              VkRect2D bin,
+                              VkOffset2D common_bin_offset,
                               unsigned views,
-                              const VkExtent2D *frag_areas)
+                              const VkExtent2D *frag_areas,
+                              const VkRect2D *bins)
 {
    const struct apply_sysmem_clear_coords_state *state =
       (const struct apply_sysmem_clear_coords_state *)data;
    assert(state->view < views);
 
    VkExtent2D frag_area = frag_areas[state->view];
+   VkRect2D bin = bins[state->view];
 
-   VkOffset2D offset = tu_fdm_per_bin_offset(frag_area, bin);
+   VkOffset2D offset = tu_fdm_per_bin_offset(frag_area, bin, common_bin_offset);
 
    unsigned x1 = state->rect.offset.x / frag_area.width + offset.x;
    unsigned x2 = DIV_ROUND_UP(state->rect.offset.x + state->rect.extent.width,
@@ -4182,17 +4200,19 @@ static void
 fdm_apply_gmem_clear_coords(struct tu_cmd_buffer *cmd,
                             struct tu_cs *cs,
                             void *data,
-                            VkRect2D bin,
+                            VkOffset2D common_bin_offset,
                             unsigned views,
-                            const VkExtent2D *frag_areas)
+                            const VkExtent2D *frag_areas,
+                            const VkRect2D *bins)
 {
    const struct apply_gmem_clear_coords_state *state =
       (const struct apply_gmem_clear_coords_state *)data;
    assert(state->view < views);
 
    VkExtent2D frag_area = frag_areas[state->view];
+   VkRect2D bin = bins[state->view];
 
-   VkOffset2D offset = tu_fdm_per_bin_offset(frag_area, bin);
+   VkOffset2D offset = tu_fdm_per_bin_offset(frag_area, bin, common_bin_offset);
 
    unsigned x1 = state->rect.offset.x / frag_area.width + offset.x;
    unsigned x2 = DIV_ROUND_UP(state->rect.offset.x + state->rect.extent.width,
@@ -4816,14 +4836,16 @@ static void
 fdm_apply_load_coords(struct tu_cmd_buffer *cmd,
                       struct tu_cs *cs,
                       void *data,
-                      VkRect2D bin,
+                      VkOffset2D common_bin_offset,
                       unsigned views,
-                      const VkExtent2D *frag_areas)
+                      const VkExtent2D *frag_areas,
+                      const VkRect2D *bins)
 {
    const struct apply_load_coords_state *state =
       (const struct apply_load_coords_state *)data;
    assert(state->view < views);
    VkExtent2D frag_area = frag_areas[state->view];
+   VkRect2D bin = bins[state->view];
 
    assert(bin.extent.width % frag_area.width == 0);
    assert(bin.extent.height % frag_area.height == 0);
@@ -4831,10 +4853,10 @@ fdm_apply_load_coords(struct tu_cmd_buffer *cmd,
    uint32_t scaled_height = bin.extent.height / frag_area.height;
 
    const float coords[] = {
-      bin.offset.x,                    bin.offset.y,
-      bin.offset.x,                    bin.offset.y,
-      bin.offset.x + scaled_width,     bin.offset.y + scaled_height,
-      bin.offset.x + bin.extent.width, bin.offset.y + bin.extent.height,
+      common_bin_offset.x,                common_bin_offset.y,
+      bin.offset.x,                       bin.offset.y,
+      common_bin_offset.x + scaled_width, common_bin_offset.y + scaled_height,
+      bin.offset.x + bin.extent.width,    bin.offset.y + bin.extent.height,
    };
    r3d_coords_raw(cmd, cs, coords);
 }
@@ -5050,6 +5072,19 @@ store_cp_blit(struct tu_cmd_buffer *cmd,
    enum a6xx_format format = fmt.fmt;
    fixup_src_format(&src_format, dst_format, &format);
 
+   uint32_t src_width = dst_iview->vk.extent.width;
+   uint32_t src_height = dst_iview->vk.extent.height;
+
+   /* With FDM offset, we may blit from an extra row/column of tiles whose
+    * source coordinates are outside of the attachment. Add an extra tile
+    * width/height to the size to avoid clipping the source.
+    */
+   if (tu_enable_fdm_offset(cmd)) {
+      const struct tu_tiling_config *tiling = cmd->state.tiling;
+      src_width += tiling->tile0.width;
+      src_height += tiling->tile0.height;
+   }
+
    tu_cs_emit_regs(cs,
                    SP_PS_2D_SRC_INFO(CHIP,
                       .color_format = format,
@@ -5063,8 +5098,8 @@ store_cp_blit(struct tu_cmd_buffer *cmd,
                       .unk22 = 1,
                       .mutableen = src_iview->view.is_mutable),
                    SP_PS_2D_SRC_SIZE(CHIP,
-                      .width = dst_iview->vk.extent.width,
-                      .height = dst_iview->vk.extent.height),
+                      .width = src_width,
+                      .height = src_height),
                    SP_PS_2D_SRC(CHIP, .qword = cmd->device->physical_device->gmem_base + gmem_offset),
                    SP_PS_2D_SRC_PITCH(CHIP, .pitch = cmd->state.tiling->tile0.width * cpp));
 
@@ -5274,14 +5309,16 @@ static void
 fdm_apply_store_coords(struct tu_cmd_buffer *cmd,
                        struct tu_cs *cs,
                        void *data,
-                       VkRect2D bin,
+                       VkOffset2D common_bin_offset,
                        unsigned views,
-                       const VkExtent2D *frag_areas)
+                       const VkExtent2D *frag_areas,
+                       const VkRect2D *bins)
 {
    const struct apply_store_coords_state *state =
       (const struct apply_store_coords_state *)data;
    assert(state->view < views);
    VkExtent2D frag_area = frag_areas[state->view];
+   VkRect2D bin = bins[state->view];
 
    /* The bin width/height must be a multiple of the frag_area to make sure
     * that the scaling happens correctly. This means there may be some
@@ -5299,10 +5336,10 @@ fdm_apply_store_coords(struct tu_cmd_buffer *cmd,
       A6XX_GRAS_2D_DST_BR(.x = bin.offset.x + bin.extent.width - 1,
                           .y = bin.offset.y + bin.extent.height - 1));
    tu_cs_emit_regs(cs,
-                   A6XX_GRAS_2D_SRC_TL_X(bin.offset.x),
-                   A6XX_GRAS_2D_SRC_BR_X(bin.offset.x + scaled_width - 1),
-                   A6XX_GRAS_2D_SRC_TL_Y(bin.offset.y),
-                   A6XX_GRAS_2D_SRC_BR_Y(bin.offset.y + scaled_height - 1));
+                   A6XX_GRAS_2D_SRC_TL_X(common_bin_offset.x),
+                   A6XX_GRAS_2D_SRC_BR_X(common_bin_offset.x + scaled_width - 1),
+                   A6XX_GRAS_2D_SRC_TL_Y(common_bin_offset.y),
+                   A6XX_GRAS_2D_SRC_BR_Y(common_bin_offset.y + scaled_height - 1));
 }
 
 template <chip CHIP>
diff --git a/src/freedreno/vulkan/tu_cmd_buffer.cc b/src/freedreno/vulkan/tu_cmd_buffer.cc
index 1f05a9d622821..6cd9b01759f19 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.cc
+++ b/src/freedreno/vulkan/tu_cmd_buffer.cc
@@ -700,7 +700,8 @@ tu6_emit_render_cntl<A7XX>(struct tu_cmd_buffer *cmd,
 }
 
 static void
-tu6_emit_blit_scissor(struct tu_cmd_buffer *cmd, struct tu_cs *cs, bool align)
+tu6_emit_blit_scissor(struct tu_cmd_buffer *cmd, struct tu_cs *cs, bool align,
+                      bool used_by_sysmem)
 {
    struct tu_physical_device *phys_dev = cmd->device->physical_device;
    const VkRect2D *render_area = &cmd->state.render_area;
@@ -727,9 +728,42 @@ tu6_emit_blit_scissor(struct tu_cmd_buffer *cmd, struct tu_cs *cs, bool align)
       y2 = ALIGN_POT(y2 + 1, phys_dev->info->gmem_align_h) - 1;
    }
 
-   tu_cs_emit_regs(cs,
-                   A6XX_RB_BLIT_SCISSOR_TL(.x = x1, .y = y1),
-                   A6XX_RB_BLIT_SCISSOR_BR(.x = x2, .y = y2));
+   /* With FDM offset, bins are shifted to the right in GMEM space compared to
+    * framebuffer space. We do not use RB_BLIT_SCISSOR_* for loads and stores
+    * because those do not use the fast path, but we do use it for
+    * LOAD_OP_CLEAR. Expand the render area so that GMEM clears work
+    * correctly. We may over-clear but that's ok because the store is clipped
+    * to the render area.
+    */
+   if (tu_enable_fdm_offset(cmd)) {
+      const struct tu_tiling_config *tiling = cmd->state.tiling;
+
+      /* If this is a generic clear that's also used in sysmem mode then we
+       * need to emit the unmodified render area in sysmem mode because
+       * over-clearing is not allowed.
+       */
+      if (used_by_sysmem) {
+         tu_cs_emit_regs(cs,
+                         A6XX_RB_BLIT_SCISSOR_TL(.x = x1, .y = y1),
+                         A6XX_RB_BLIT_SCISSOR_BR(.x = x2, .y = y2));
+         tu_cond_exec_start(cs, CP_COND_REG_EXEC_0_MODE(RENDER_MODE) |
+                                CP_COND_REG_EXEC_0_GMEM);
+      }
+
+      x2 += tiling->tile0.width;
+      y2 += tiling->tile0.height;
+      tu_cs_emit_regs(cs,
+                      A6XX_RB_BLIT_SCISSOR_TL(.x = x1, .y = y1),
+                      A6XX_RB_BLIT_SCISSOR_BR(.x = x2, .y = y2));
+
+      if (used_by_sysmem) {
+         tu_cond_exec_end(cs);
+      }
+   } else {
+      tu_cs_emit_regs(cs,
+                      A6XX_RB_BLIT_SCISSOR_TL(.x = x1, .y = y1),
+                      A6XX_RB_BLIT_SCISSOR_BR(.x = x2, .y = y2));
+   }
 }
 
 void
@@ -950,12 +984,20 @@ tu6_update_msaa_disable(struct tu_cmd_buffer *cmd)
    }
 }
 
+static const struct tu_vsc_config *
+tu_vsc_config(struct tu_cmd_buffer *cmd, const struct tu_tiling_config *tiling)
+{
+   if (tu_enable_fdm_offset(cmd))
+      return &tiling->fdm_offset_vsc;
+   return &tiling->vsc;
+}
+
 static bool
 use_hw_binning(struct tu_cmd_buffer *cmd)
 {
    const struct tu_framebuffer *fb = cmd->state.framebuffer;
    const struct tu_tiling_config *tiling = &fb->tiling[cmd->state.gmem_layout];
-   const struct tu_vsc_config *vsc = &tiling->vsc;
+   const struct tu_vsc_config *vsc = tu_vsc_config(cmd, tiling);
 
    /* XFB commands are emitted for BINNING || SYSMEM, which makes it
     * incompatible with non-hw binning GMEM rendering. this is required because
@@ -1019,7 +1061,7 @@ use_sysmem_rendering(struct tu_cmd_buffer *cmd,
       return true;
    }
 
-   const struct tu_vsc_config *vsc = &cmd->state.tiling->vsc;
+   const struct tu_vsc_config *vsc = tu_vsc_config(cmd, cmd->state.tiling);
 
    /* XFB is incompatible with non-hw binning GMEM rendering, see use_hw_binning */
    if (cmd->state.rp.xfb_used && !vsc->binning_possible) {
@@ -1064,7 +1106,7 @@ static void
 tu6_emit_cond_for_load_stores(struct tu_cmd_buffer *cmd, struct tu_cs *cs,
                               uint32_t pipe, uint32_t slot, bool skip_wfm)
 {
-   const struct tu_vsc_config *vsc = &cmd->state.tiling->vsc;
+   const struct tu_vsc_config *vsc = tu_vsc_config(cmd, cmd->state.tiling);
 
    if (vsc->binning_possible &&
        cmd->state.pass->has_cond_load_store) {
@@ -1085,16 +1127,48 @@ struct tu_tile_config {
    VkExtent2D frag_areas[MAX_VIEWS];
 };
 
+/* For bin offsetting we want to do "Euclidean division," where the remainder
+ * (i.e. the offset of the bin) is always positive. Unfortunately C/C++
+ * remainder and division don't do this, so we have to implement it ourselves.
+ *
+ * For example, we should have:
+ *
+ * euclid_rem(-3, 4) = 1
+ * euclid_rem(-4, 4) = 0
+ * euclid_rem(-4, 4) = 3
+ */
+
+static int32_t
+euclid_rem(int32_t divisor, int32_t divisend)
+{
+   if (divisor >= 0)
+      return divisor % divisend;
+   int32_t tmp = divisend - (-divisor % divisend);
+   return tmp == divisend ? 0 : tmp;
+}
+
+/* Calculate how much the bins for a given view should be shifted to the left
+ * and upwards, given the application-provided FDM offset.
+ */
+static VkOffset2D
+tu_bin_offset(VkOffset2D fdm_offset, const struct tu_tiling_config *tiling)
+{
+   return (VkOffset2D) {
+      euclid_rem(-fdm_offset.x, tiling->tile0.width),
+      euclid_rem(-fdm_offset.y, tiling->tile0.height),
+   };
+}
+
 template <chip CHIP>
 static void
 tu6_emit_tile_select(struct tu_cmd_buffer *cmd,
                      struct tu_cs *cs,
                      const struct tu_tile_config *tile,
-                     bool fdm)
+                     bool fdm, const VkOffset2D *fdm_offsets)
 {
    struct tu_physical_device *phys_dev = cmd->device->physical_device;
    const struct tu_tiling_config *tiling = cmd->state.tiling;
-   const struct tu_vsc_config *vsc = &tiling->vsc;
+   const struct tu_vsc_config *vsc = tu_vsc_config(cmd, tiling);
    bool hw_binning = use_hw_binning(cmd);
 
    tu_cs_emit_pkt7(cs, CP_SET_MARKER, 1);
@@ -1123,6 +1197,7 @@ tu6_emit_tile_select(struct tu_cmd_buffer *cmd,
 
    const uint32_t x1 = tiling->tile0.width * tile->pos.x;
    const uint32_t y1 = tiling->tile0.height * tile->pos.y;
+
    const uint32_t x2 = MIN2(x1 + tiling->tile0.width, MAX_VIEWPORT_SIZE);
    const uint32_t y2 = MIN2(y1 + tiling->tile0.height, MAX_VIEWPORT_SIZE);
    tu6_emit_window_scissor(cs, x1, y1, x2 - 1, y2 - 1);
@@ -1166,11 +1241,29 @@ tu6_emit_tile_select(struct tu_cmd_buffer *cmd,
          { x1, y1 },
          { (x2 - x1) * tile->extent.width, (y2 - y1) * tile->extent.height }
       };
+      VkRect2D bins[views];
+      for (unsigned i = 0; i < views; i++) {
+         if (!fdm_offsets) {
+            bins[i] = bin;
+            continue;
+         }
+
+         VkOffset2D bin_offset = tu_bin_offset(fdm_offsets[i], tiling);
+
+         bins[i].offset.x = MAX2(0, (int32_t)x1 - bin_offset.x);
+         bins[i].offset.y = MAX2(0, (int32_t)y1 - bin_offset.y);
+         bins[i].extent.width =
+            MAX2(MIN2((int32_t)x1 + bin.extent.width - bin_offset.x, MAX_VIEWPORT_SIZE) - bins[i].offset.x, 0);
+         bins[i].extent.height =
+            MAX2(MIN2((int32_t)y1 + bin.extent.height - bin_offset.y, MAX_VIEWPORT_SIZE) - bins[i].offset.y, 0);
+      }
+
       util_dynarray_foreach (&cmd->fdm_bin_patchpoints,
                              struct tu_fdm_bin_patchpoint, patch) {
          tu_cs_emit_pkt7(cs, CP_MEM_WRITE, 2 + patch->size);
          tu_cs_emit_qw(cs, patch->iova);
-         patch->apply(cmd, cs, patch->data, bin, views, tile->frag_areas);
+         patch->apply(cmd, cs, patch->data, (VkOffset2D) { x1, y1 }, views,
+                      tile->frag_areas, bins);
       }
 
       /* Make the CP wait until the CP_MEM_WRITE's to the command buffers
@@ -1257,7 +1350,7 @@ tu6_emit_tile_store(struct tu_cmd_buffer *cmd, struct tu_cs *cs)
    const struct tu_render_pass *pass = cmd->state.pass;
    const struct tu_subpass *subpass = &pass->subpasses[pass->subpass_count-1];
    const struct tu_framebuffer *fb = cmd->state.framebuffer;
-   const struct tu_vsc_config *vsc = &cmd->state.tiling->vsc;
+   const struct tu_vsc_config *vsc = tu_vsc_config(cmd, cmd->state.tiling);
 
    if (pass->has_fdm)
       tu_cs_set_writeable(cs, true);
@@ -1266,7 +1359,7 @@ tu6_emit_tile_store(struct tu_cmd_buffer *cmd, struct tu_cs *cs)
    tu_cs_emit(cs, A6XX_CP_SET_MARKER_0_MODE(RM6_BIN_RESOLVE) |
                   A6XX_CP_SET_MARKER_0_USES_GMEM);
 
-   tu6_emit_blit_scissor(cmd, cs, true);
+   tu6_emit_blit_scissor(cmd, cs, true, false);
 
    struct tu_resolve_group resolve_group = {};
 
@@ -1651,13 +1744,31 @@ tu6_init_hw(struct tu_cmd_buffer *cmd, struct tu_cs *cs)
    tu_cs_sanity_check(cs);
 }
 
+bool
+tu_enable_fdm_offset(struct tu_cmd_buffer *cmd)
+{
+   if (!cmd->state.pass)
+      return false;
+
+   if (!cmd->state.pass->has_fdm)
+      return false;
+
+   unsigned fdm_a = cmd->state.pass->fragment_density_map.attachment;
+   if (fdm_a == VK_ATTACHMENT_UNUSED)
+      return TU_DEBUG(FDM_OFFSET);
+
+   const struct tu_image_view *fdm = cmd->state.attachments[fdm_a];
+   return fdm->image->vk.create_flags &
+      VK_IMAGE_CREATE_FRAGMENT_DENSITY_MAP_OFFSET_BIT_QCOM;
+}
+
 static void
 update_vsc_pipe(struct tu_cmd_buffer *cmd,
                 struct tu_cs *cs,
                 uint32_t num_vsc_pipes)
 {
    const struct tu_tiling_config *tiling = cmd->state.tiling;
-   const struct tu_vsc_config *vsc = &tiling->vsc;
+   const struct tu_vsc_config *vsc = tu_vsc_config(cmd, tiling);
 
    tu_cs_emit_regs(cs,
                    A6XX_VSC_BIN_SIZE(.width = tiling->tile0.width,
@@ -1685,7 +1796,7 @@ static void
 emit_vsc_overflow_test(struct tu_cmd_buffer *cmd, struct tu_cs *cs)
 {
    const struct tu_tiling_config *tiling = cmd->state.tiling;
-   const struct tu_vsc_config *vsc = &tiling->vsc;
+   const struct tu_vsc_config *vsc = tu_vsc_config(cmd, tiling);
    const uint32_t used_pipe_count =
       vsc->pipe_count.width * vsc->pipe_count.height;
 
@@ -1716,36 +1827,70 @@ emit_vsc_overflow_test(struct tu_cmd_buffer *cmd, struct tu_cs *cs)
 
 template <chip CHIP>
 static void
-tu6_emit_binning_pass(struct tu_cmd_buffer *cmd, struct tu_cs *cs)
+tu6_emit_binning_pass(struct tu_cmd_buffer *cmd, struct tu_cs *cs,
+                      const VkOffset2D *fdm_offsets)
 {
    struct tu_physical_device *phys_dev = cmd->device->physical_device;
    const struct tu_framebuffer *fb = cmd->state.framebuffer;
+   const struct tu_tiling_config *tiling = cmd->state.tiling;
 
    /* If this command buffer may be executed multiple times, then
     * viewports/scissor states may have been changed by previous executions
-    * and we need to reset them before executing the binning IB.
+    * and we need to reset them before executing the binning IB. With FDM
+    * offset the viewport also needs to be transformed during the binning
+    * phase.
     */
-   if (!(cmd->usage_flags & VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT) &&
-       cmd->fdm_bin_patchpoints.size != 0) {
+   if ((!(cmd->usage_flags & VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT) ||
+        fdm_offsets) && cmd->fdm_bin_patchpoints.size != 0) {
       unsigned num_views = MAX2(cmd->state.pass->num_views, 1);
       VkExtent2D unscaled_frag_areas[num_views];
-      for (unsigned i = 0; i < num_views; i++)
+      VkRect2D bins[num_views];
+      for (unsigned i = 0; i < num_views; i++) {
          unscaled_frag_areas[i] = (VkExtent2D) { 1, 1 };
-      VkRect2D bin = { { 0, 0 }, { fb->width, fb->height } };
+         if (fdm_offsets && !cmd->state.rp.shared_viewport) {
+            /* We need to shift over the viewport and scissor during the
+             * binning pass to match the shift applied when rendering. The way
+             * to do this is to make the per-view bin start negative. In the
+             * actual rendering pass, the per-view bin start is shifted in a
+             * negative direction but the first bin is clipped so that the bin
+             * start is never negative, but we need to do this to avoid
+             * clipping the user scissor to a non-zero common bin start. We
+             * skip patching load/store below in order to avoid patching loads
+             * and stores to a crazy negative-offset bin. The parts of the
+             * framebuffer left or above the origin correspond to the
+             * non-visible parts of the left or top bins that will be
+             * discarded. The framebuffer still needs to extend to the
+             * original bottom and right, to avoid incorrectly clipping the
+             * user scissor, so we need to add to the width and height to
+             * compensate.
+             */
+            VkOffset2D bin_offset = tu_bin_offset(fdm_offsets[i], tiling);
+            bins[i] = {
+               { -bin_offset.x, -bin_offset.y },
+               { fb->width + bin_offset.x, fb->height + bin_offset.y },
+            };
+         } else {
+            bins[i] = { { 0, 0 }, { fb->width, fb->height } };
+         }
+      }
       util_dynarray_foreach (&cmd->fdm_bin_patchpoints,
                              struct tu_fdm_bin_patchpoint, patch) {
          if (patch->flags & TU_FDM_SKIP_BINNING)
             continue;
          tu_cs_emit_pkt7(cs, CP_MEM_WRITE, 2 + patch->size);
          tu_cs_emit_qw(cs, patch->iova);
-         patch->apply(cmd, cs, patch->data, bin, num_views, unscaled_frag_areas);
+         patch->apply(cmd, cs, patch->data, (VkOffset2D) {0, 0}, num_views,
+                      unscaled_frag_areas, bins);
       }
 
       tu_cs_emit_pkt7(cs, CP_WAIT_MEM_WRITES, 0);
       tu_cs_emit_pkt7(cs, CP_WAIT_FOR_ME, 0);
    }
 
-   tu6_emit_window_scissor(cs, 0, 0, fb->width - 1, fb->height - 1);
+   uint32_t width = fb->width + (fdm_offsets ? tiling->tile0.width : 0);
+   uint32_t height = fb->height + (fdm_offsets ? tiling->tile0.height : 0);
+
+   tu6_emit_window_scissor(cs, 0, 0, width - 1, height - 1);
 
    tu_cs_emit_pkt7(cs, CP_SET_MARKER, 1);
    tu_cs_emit(cs, A6XX_CP_SET_MARKER_0_MODE(RM6_BIN_VISIBILITY));
@@ -1934,6 +2079,22 @@ tu_emit_input_attachments(struct tu_cmd_buffer *cmd,
       if (!iview->view.is_mutable)
          dst[0] &= ~A6XX_TEX_CONST_0_SWAP__MASK;
       dst[0] |= A6XX_TEX_CONST_0_TILE_MODE(TILE6_2);
+
+      /* If FDM offset is used, the last row and column extend beyond the
+       * framebuffer but are shifted over when storing. Expand the width and
+       * height to account for that.
+       */
+      if (tu_enable_fdm_offset(cmd)) {
+         uint32_t width = dst[1] & A6XX_TEX_CONST_1_WIDTH__MASK;
+         uint32_t height = (dst[1] & A6XX_TEX_CONST_1_HEIGHT__MASK) >>
+            A6XX_TEX_CONST_1_HEIGHT__SHIFT;
+         width += cmd->state.tiling->tile0.width;
+         height += cmd->state.tiling->tile0.height;
+         dst[1] = (dst[1] & ~(A6XX_TEX_CONST_1_WIDTH__MASK |
+                              A6XX_TEX_CONST_1_HEIGHT__MASK)) |
+            A6XX_TEX_CONST_1_WIDTH(width) | A6XX_TEX_CONST_1_HEIGHT(height);
+      }
+
       dst[2] =
          A6XX_TEX_CONST_2_TYPE(A6XX_TEX_2D) |
          A6XX_TEX_CONST_2_PITCH(tiling->tile0.width * cpp);
@@ -2172,11 +2333,12 @@ tu6_sysmem_render_end(struct tu_cmd_buffer *cmd, struct tu_cs *cs,
 template <chip CHIP>
 static void
 tu6_tile_render_begin(struct tu_cmd_buffer *cmd, struct tu_cs *cs,
-                      struct tu_renderpass_result *autotune_result)
+                      struct tu_renderpass_result *autotune_result,
+                      const VkOffset2D *fdm_offsets)
 {
    struct tu_physical_device *phys_dev = cmd->device->physical_device;
    const struct tu_tiling_config *tiling = cmd->state.tiling;
-   const struct tu_vsc_config *vsc = &tiling->vsc;
+   const struct tu_vsc_config *vsc = tu_vsc_config(cmd, tiling);
    tu_lrz_tiling_begin<CHIP>(cmd, cs);
 
    tu_cs_emit_pkt7(cs, CP_SKIP_IB2_ENABLE_GLOBAL, 1);
@@ -2220,7 +2382,7 @@ tu6_tile_render_begin(struct tu_cmd_buffer *cmd, struct tu_cs *cs,
 
       tu6_emit_render_cntl<CHIP>(cmd, cmd->state.subpass, cs, true);
 
-      tu6_emit_binning_pass<CHIP>(cmd, cs);
+      tu6_emit_binning_pass<CHIP>(cmd, cs, fdm_offsets);
 
       if (CHIP == A6XX) {
          tu_cs_emit_regs(cs,
@@ -2265,9 +2427,9 @@ template <chip CHIP>
 static void
 tu6_render_tile(struct tu_cmd_buffer *cmd, struct tu_cs *cs,
                 const struct tu_tile_config *tile,
-                bool fdm)
+                bool fdm, const VkOffset2D *fdm_offsets)
 {
-   tu6_emit_tile_select<CHIP>(cmd, &cmd->cs, tile, fdm);
+   tu6_emit_tile_select<CHIP>(cmd, &cmd->cs, tile, fdm, fdm_offsets);
    tu_lrz_before_tile<CHIP>(cmd, &cmd->cs);
 
    trace_start_draw_ib_gmem(&cmd->trace, &cmd->cs);
@@ -2333,7 +2495,8 @@ tu6_tile_render_end(struct tu_cmd_buffer *cmd, struct tu_cs *cs,
 static void
 tu_calc_frag_area(struct tu_cmd_buffer *cmd,
                   struct tu_tile_config *tile,
-                  const struct tu_image_view *fdm)
+                  const struct tu_image_view *fdm,
+                  const VkOffset2D *fdm_offsets)
 {
    const struct tu_tiling_config *tiling = cmd->state.tiling;
    const uint32_t x1 = tiling->tile0.width * tile->pos.x;
@@ -2346,11 +2509,68 @@ tu_calc_frag_area(struct tu_cmd_buffer *cmd,
    const struct tu_framebuffer *fb = cmd->state.framebuffer;
    struct tu_frag_area raw_areas[views];
    if (fdm) {
-      tu_fragment_density_map_sample(fdm,
-                                     (x1 + MIN2(x2, fb->width)) / 2,
-                                     (y1 + MIN2(y2, fb->height)) / 2,
-                                     fb->width, fb->height, views,
-                                     raw_areas);
+      for (unsigned i = 0; i < views; i++) {
+         VkOffset2D sample_pos = { 0, 0 };
+
+         /* Offsets less than a tile size are accomplished by sliding the
+          * tiles.  However once we shift a whole tile size then we reset the
+          * tiles back to where they were at the beginning and we need to
+          * adjust where each bin is sampling from:
+          *
+          * x offset = 0:
+          *
+          * ------------------------------------
+          * |   *   |   *   |   *   | (unused) |
+          * ------------------------------------
+          *
+          * x offset = 4:
+          *
+          * -------------------------
+          * | * |   *   |   *   | * |
+          * -------------------------
+          *
+          * x offset = 8:
+          *
+          * ------------------------------------
+          * |   *   |   *   |   *   | (unused) |
+          * ------------------------------------
+          *
+          * Note that as the offset increases we actually have to slide the
+          * tiles to the left, until we reach the whole tile size and reset
+          * the tile positions.
+          *
+          * If we were forced to use a shared viewport, then we must not shift
+          * over the tiles and instead must shift when sampling because we
+          * cannot shift the tiles differently per view.
+          *
+          * Note that we cannot clamp x2/y2 to the framebuffer size, as we
+          * normally would do, because then tiles along the edge would
+          * incorrectly nudge the sample_pos towards the center of the
+          * framebuffer. If we shift one complete tile over towards the
+          * center and reset the tiles as above, the sample_pos would
+          * then shift back towards the edge and we could get a "pop" from
+          * suddenly changing density due to the slight shift.
+          */
+         if (fdm_offsets) {
+            VkOffset2D offset = fdm_offsets[i];
+            if (!cmd->state.rp.shared_viewport) {
+               VkOffset2D bin_offset = tu_bin_offset(fdm_offsets[i], tiling);
+               offset.x += bin_offset.x;
+               offset.y += bin_offset.y;
+            }
+            sample_pos.x = (x1 + x2) / 2 - offset.x;
+            sample_pos.y = (y1 + y2) / 2 - offset.y;
+         } else {
+            sample_pos.x = (x1 + MIN2(x2, fb->width)) / 2;
+            sample_pos.y = (y1 + MIN2(y2, fb->height)) / 2;
+         }
+
+         tu_fragment_density_map_sample(fdm,
+                                        sample_pos.x,
+                                        sample_pos.y,
+                                        fb->width, fb->height, i,
+                                        &raw_areas[i]);
+      }
    } else {
       for (unsigned i = 0; i < views; i++)
          raw_areas[i].width = raw_areas[i].height = 1.0f;
@@ -2383,10 +2603,24 @@ tu_calc_frag_area(struct tu_cmd_buffer *cmd,
       width = 1u << util_logbase2(width);
       height = 1u << util_logbase2(height);
 
+      /* When FDM offset is enabled, the fragment area has to divide the
+       * offset to make sure that we don't have tiles with partial fragments.
+       * It would be bad to have the fragment area change as a function of the
+       * offset, because we'd get "popping" as the resolution changes with the
+       * offset, so just make sure it divides the offset granularity. This
+       * should mean it always divides the offset for any possible offset.
+       */
+      if (fdm_offsets) {
+         width = MIN2(width, TU_FDM_OFFSET_GRANULARITY);
+         height = MIN2(height, TU_FDM_OFFSET_GRANULARITY);
+      }
+
       /* Make sure that the width/height divides the tile width/height so
        * we don't have to do extra awkward clamping of the edges of each
-       * bin when resolving. Note that because the tile width is rounded to
-       * a multiple of 32 any power of two 32 or less will work.
+       * bin when resolving. It also has to divide the fdm offset, if any.
+       * Note that because the tile width is rounded to a multiple of 32 any
+       * power of two 32 or less will work, and if there is an offset then it
+       * must be a multiple of 4 so 2 or 4 will definitely work.
        *
        * TODO: Try to take advantage of the total area allowance here, too.
        */
@@ -2481,7 +2715,8 @@ template <chip CHIP>
 void
 tu_render_pipe_fdm(struct tu_cmd_buffer *cmd, uint32_t pipe,
                    uint32_t tx1, uint32_t ty1, uint32_t tx2, uint32_t ty2,
-                   const struct tu_image_view *fdm)
+                   const struct tu_image_view *fdm,
+                   const VkOffset2D *fdm_offsets)
 {
    uint32_t width = tx2 - tx1;
    uint32_t height = ty2 - ty1;
@@ -2500,7 +2735,7 @@ tu_render_pipe_fdm(struct tu_cmd_buffer *cmd, uint32_t pipe,
          tile->extent = { 1, 1 };
          tile->pipe = pipe;
          tile->slot_mask = 1u << (width * y + x);
-         tu_calc_frag_area(cmd, tile, fdm);
+         tu_calc_frag_area(cmd, tile, fdm, fdm_offsets);
       }
    }
 
@@ -2544,7 +2779,8 @@ tu_render_pipe_fdm(struct tu_cmd_buffer *cmd, uint32_t pipe,
          if (merged_tiles & (1u << tile_idx))
             continue;
 
-         tu6_render_tile<CHIP>(cmd, &cmd->cs, &tiles[tile_idx], true);
+         tu6_render_tile<CHIP>(cmd, &cmd->cs, &tiles[tile_idx],
+                               true, fdm_offsets);
       }
    }
 }
@@ -2552,10 +2788,11 @@ tu_render_pipe_fdm(struct tu_cmd_buffer *cmd, uint32_t pipe,
 template <chip CHIP>
 static void
 tu_cmd_render_tiles(struct tu_cmd_buffer *cmd,
-                    struct tu_renderpass_result *autotune_result)
+                    struct tu_renderpass_result *autotune_result,
+                    const VkOffset2D *fdm_offsets)
 {
    const struct tu_tiling_config *tiling = cmd->state.tiling;
-   const struct tu_vsc_config *vsc = &tiling->vsc;
+   const struct tu_vsc_config *vsc = tu_vsc_config(cmd, tiling);
    const struct tu_image_view *fdm = NULL;
 
    if (cmd->state.pass->fragment_density_map.attachment != VK_ATTACHMENT_UNUSED) {
@@ -2577,7 +2814,7 @@ tu_cmd_render_tiles(struct tu_cmd_buffer *cmd,
 
    cmd->trace_renderpass_end = u_trace_end_iterator(&cmd->trace);
 
-   tu6_tile_render_begin<CHIP>(cmd, &cmd->cs, autotune_result);
+   tu6_tile_render_begin<CHIP>(cmd, &cmd->cs, autotune_result, fdm_offsets);
 
    /* Note: we reverse the order of walking the pipes and tiles on every
     * other row, to improve texture cache locality compared to raster order.
@@ -2597,7 +2834,8 @@ tu_cmd_render_tiles(struct tu_cmd_buffer *cmd,
          uint32_t ty2 = MIN2(ty1 + vsc->pipe0.height, vsc->tile_count.height);
 
          if (merge_tiles) {
-            tu_render_pipe_fdm<CHIP>(cmd, pipe, tx1, ty1, tx2, ty2, fdm);
+            tu_render_pipe_fdm<CHIP>(cmd, pipe, tx1, ty1, tx2, ty2, fdm,
+                                     fdm_offsets);
             continue;
          }
 
@@ -2618,9 +2856,10 @@ tu_cmd_render_tiles(struct tu_cmd_buffer *cmd,
                   .extent = { 1, 1 },
                };
                if (has_fdm)
-                  tu_calc_frag_area(cmd, &tile, fdm);
+                  tu_calc_frag_area(cmd, &tile, fdm, fdm_offsets);
 
-               tu6_render_tile<CHIP>(cmd, &cmd->cs, &tile, has_fdm);
+               tu6_render_tile<CHIP>(cmd, &cmd->cs, &tile, has_fdm,
+                                     fdm_offsets);
             }
             slot_row += tile_row_stride;
          }
@@ -2671,7 +2910,8 @@ tu_cmd_render_sysmem(struct tu_cmd_buffer *cmd,
 
 template <chip CHIP>
 void
-tu_cmd_render(struct tu_cmd_buffer *cmd_buffer)
+tu_cmd_render(struct tu_cmd_buffer *cmd_buffer,
+              const VkOffset2D *fdm_offsets)
 {
    if (cmd_buffer->state.rp.has_tess)
       tu6_lazy_emit_tessfactor_addr<CHIP>(cmd_buffer);
@@ -2680,7 +2920,7 @@ tu_cmd_render(struct tu_cmd_buffer *cmd_buffer)
    if (use_sysmem_rendering(cmd_buffer, &autotune_result))
       tu_cmd_render_sysmem<CHIP>(cmd_buffer, autotune_result);
    else
-      tu_cmd_render_tiles<CHIP>(cmd_buffer, autotune_result);
+      tu_cmd_render_tiles<CHIP>(cmd_buffer, autotune_result, fdm_offsets);
 
    /* Outside of renderpasses we assume all draw states are disabled. We do
     * this outside the draw CS for the normal case where 3d gmem stores aren't
@@ -4753,7 +4993,7 @@ tu_CmdExecuteCommands(VkCommandBuffer commandBuffer,
                    */
                   tu_restore_suspended_pass(cmd, cmd);
 
-                  TU_CALLX(cmd->device, tu_cmd_render)(cmd);
+                  TU_CALLX(cmd->device, tu_cmd_render)(cmd, NULL);
                   if (cmd->state.suspend_resume == SR_IN_CHAIN)
                      cmd->state.suspend_resume = SR_NONE;
                   else
@@ -4859,7 +5099,7 @@ tu_emit_subpass_begin_gmem(struct tu_cmd_buffer *cmd, struct tu_resolve_group *r
 {
    struct tu_cs *cs = &cmd->draw_cs;
    uint32_t subpass_idx = cmd->state.subpass - cmd->state.pass->subpasses;
-   const struct tu_vsc_config *vsc = &cmd->state.tiling->vsc;
+   const struct tu_vsc_config *vsc = tu_vsc_config(cmd, cmd->state.tiling);
 
    /* If we might choose to bin, then put the loads under a check for geometry
     * having been binned to this tile.  If we don't choose to bin in the end,
@@ -4884,7 +5124,7 @@ tu_emit_subpass_begin_gmem(struct tu_cmd_buffer *cmd, struct tu_resolve_group *r
       struct tu_render_pass_attachment *att = &cmd->state.pass->attachments[i];
       if ((att->load || att->load_stencil) && att->first_subpass_idx == subpass_idx) {
          if (!emitted_scissor) {
-            tu6_emit_blit_scissor(cmd, cs, true);
+            tu6_emit_blit_scissor(cmd, cs, true, false);
             emitted_scissor = true;
          }
          tu_load_gmem_attachment<CHIP>(cmd, cs, resolve_group, i,
@@ -4900,7 +5140,7 @@ tu_emit_subpass_begin_gmem(struct tu_cmd_buffer *cmd, struct tu_resolve_group *r
             &cmd->state.pass->attachments[i];
          if (att->clear_mask && att->first_subpass_idx == subpass_idx) {
             if (!emitted_scissor) {
-               tu6_emit_blit_scissor(cmd, cs, false);
+               tu6_emit_blit_scissor(cmd, cs, false, false);
                emitted_scissor = true;
             }
             tu_clear_gmem_attachment<CHIP>(cmd, cs, resolve_group, i);
@@ -4951,7 +5191,7 @@ tu7_emit_subpass_clear(struct tu_cmd_buffer *cmd, struct tu_resolve_group *resol
          &cmd->state.pass->attachments[i];
       if (att->clear_mask && att->first_subpass_idx == subpass_idx) {
          if (!emitted_scissor) {
-            tu6_emit_blit_scissor(cmd, cs, false);
+            tu6_emit_blit_scissor(cmd, cs, false, true);
             emitted_scissor = true;
          }
          tu7_generic_clear_attachment(cmd, cs, resolve_group, i);
@@ -5414,7 +5654,7 @@ tu_CmdNextSubpass2(VkCommandBuffer commandBuffer,
       tu_cond_exec_start(cs, CP_COND_EXEC_0_RENDER_MODE_GMEM);
 
       if (subpass->resolve_attachments) {
-         tu6_emit_blit_scissor(cmd, cs, true);
+         tu6_emit_blit_scissor(cmd, cs, true, false);
 
          struct tu_resolve_group resolve_group = {};
 
@@ -5884,9 +6124,10 @@ static void
 fdm_apply_fs_params(struct tu_cmd_buffer *cmd,
                     struct tu_cs *cs,
                     void *data,
-                    VkRect2D bin,
+                    VkOffset2D common_bin_offset,
                     unsigned views,
-                    const VkExtent2D *frag_areas)
+                    const VkExtent2D *frag_areas,
+                    const VkRect2D *bins)
 {
    const struct apply_fs_params_state *state =
       (const struct apply_fs_params_state *)data;
@@ -5895,7 +6136,8 @@ fdm_apply_fs_params(struct tu_cmd_buffer *cmd,
    for (unsigned i = 0; i < num_consts; i++) {
       assert(i < views);
       VkExtent2D area = frag_areas[i];
-      VkOffset2D offset = tu_fdm_per_bin_offset(area, bin);
+      VkRect2D bin = bins[i];
+      VkOffset2D offset = tu_fdm_per_bin_offset(area, bin, common_bin_offset);
       
       tu_cs_emit(cs, area.width);
       tu_cs_emit(cs, area.height);
@@ -7419,9 +7661,25 @@ tu_CmdEndRenderPass2(VkCommandBuffer commandBuffer,
       return;
    }
 
+   const VkSubpassFragmentDensityMapOffsetEndInfoQCOM *fdm_offset_info =
+      vk_find_struct_const(pSubpassEndInfo->pNext,
+                           SUBPASS_FRAGMENT_DENSITY_MAP_OFFSET_END_INFO_QCOM);
+   const VkOffset2D *fdm_offsets =
+      (fdm_offset_info && fdm_offset_info->fragmentDensityOffsetCount > 0) ?
+      fdm_offset_info->pFragmentDensityOffsets : NULL;
+
+   VkOffset2D test_offsets[MAX_VIEWS];
+   if (TU_DEBUG(FDM) && TU_DEBUG(FDM_OFFSET)) {
+      for (unsigned i = 0;
+           i < MAX2(cmd_buffer->state.pass->num_views, 1); i++) {
+         test_offsets[i] = { 64, 64 };
+      }
+      fdm_offsets = test_offsets;
+   }
+
    tu_cs_end(&cmd_buffer->draw_cs);
    tu_cs_end(&cmd_buffer->draw_epilogue_cs);
-   TU_CALLX(cmd_buffer->device, tu_cmd_render)(cmd_buffer);
+   TU_CALLX(cmd_buffer->device, tu_cmd_render)(cmd_buffer, fdm_offsets);
 
    cmd_buffer->state.cache.pending_flush_bits |=
       cmd_buffer->state.renderpass_cache.pending_flush_bits;
@@ -7459,7 +7717,16 @@ tu_CmdEndRendering(VkCommandBuffer commandBuffer)
           */
          tu_disable_draw_states(cmd_buffer, &cmd_buffer->cs);
       } else {
-         TU_CALLX(cmd_buffer->device, tu_cmd_render)(cmd_buffer);
+         VkOffset2D test_offsets[MAX_VIEWS];
+         const VkOffset2D *fdm_offsets = NULL;
+         if (TU_DEBUG(FDM) && TU_DEBUG(FDM_OFFSET)) {
+            for (unsigned i = 0;
+                 i < MAX2(cmd_buffer->state.pass->num_views, 1); i++) {
+               test_offsets[i] = { 64, 64 };
+            }
+            fdm_offsets = test_offsets;
+         }
+         TU_CALLX(cmd_buffer->device, tu_cmd_render)(cmd_buffer, fdm_offsets);
       }
 
       tu_reset_render_pass(cmd_buffer);
diff --git a/src/freedreno/vulkan/tu_cmd_buffer.h b/src/freedreno/vulkan/tu_cmd_buffer.h
index 3cc9f07e7db5b..b6e7d4d6c91f9 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.h
+++ b/src/freedreno/vulkan/tu_cmd_buffer.h
@@ -691,7 +691,7 @@ tu_restore_suspended_pass(struct tu_cmd_buffer *cmd,
                           struct tu_cmd_buffer *suspended);
 
 template <chip CHIP>
-void tu_cmd_render(struct tu_cmd_buffer *cmd);
+void tu_cmd_render(struct tu_cmd_buffer *cmd, const VkOffset2D *fdm_offsets);
 
 void tu_dispatch_unaligned(VkCommandBuffer commandBuffer,
                            uint32_t x, uint32_t y, uint32_t z);
@@ -744,12 +744,15 @@ void tu_disable_draw_states(struct tu_cmd_buffer *cmd, struct tu_cs *cs);
 void tu6_apply_depth_bounds_workaround(struct tu_device *device,
                                        uint32_t *rb_depth_cntl);
 
+bool tu_enable_fdm_offset(struct tu_cmd_buffer *cmd);
+
 typedef void (*tu_fdm_bin_apply_t)(struct tu_cmd_buffer *cmd,
                                    struct tu_cs *cs,
                                    void *data,
-                                   VkRect2D bin,
+                                   VkOffset2D common_bin_offset,
                                    unsigned views,
-                                   const VkExtent2D *frag_areas);
+                                   const VkExtent2D *frag_areas,
+                                   const VkRect2D *bins);
 
 enum tu_fdm_flags {
    TU_FDM_NONE = 0,
@@ -803,13 +806,15 @@ _tu_create_fdm_bin_patchpoint(struct tu_cmd_buffer *cmd,
     */
    unsigned num_views = MAX2(cmd->state.pass->num_views, 1);
    VkExtent2D unscaled_frag_areas[num_views];
+   VkRect2D bins[num_views];
    for (unsigned i = 0; i < num_views; i++) {
       unscaled_frag_areas[i] = (VkExtent2D) { 1, 1 };
-   }
-   apply(cmd, cs, state, (VkRect2D) {
+      bins[i] = (VkRect2D) {
          { 0, 0 },
          { MAX_VIEWPORT_SIZE, MAX_VIEWPORT_SIZE },
-        }, num_views, unscaled_frag_areas);
+      };
+   }
+   apply(cmd, cs, state, (VkOffset2D) {0, 0}, num_views, unscaled_frag_areas, bins);
    assert(tu_cs_get_cur_iova(cs) == patch.iova + patch.size * sizeof(uint32_t));
 
    util_dynarray_append(&cmd->fdm_bin_patchpoints,
diff --git a/src/freedreno/vulkan/tu_common.h b/src/freedreno/vulkan/tu_common.h
index af4a1aaf539f7..a8700e276c3ab 100644
--- a/src/freedreno/vulkan/tu_common.h
+++ b/src/freedreno/vulkan/tu_common.h
@@ -138,6 +138,18 @@
 #define MAX_FDM_TEXEL_SIZE_LOG2 10
 #define MAX_FDM_TEXEL_SIZE (1u << MAX_FDM_TEXEL_SIZE_LOG2)
 
+/* This granularity is arbitrary, but there are two competing concerns here:
+ * 
+ * - The fragment area has to always divide the offset, and we don't want the
+ *   fragment area changing with the offset, so we have to clamp the fragment
+ *   area to this granularity. Therefore larger granularities lead to lower
+ *   minimum resolution.
+ * - The larger the offset granularity, the choppier the motion is.
+ *
+ * Choose 8 as a compromise between the two.
+ */
+#define TU_FDM_OFFSET_GRANULARITY 8
+
 #define TU_GENX(FUNC_NAME) FD_GENX(FUNC_NAME)
 
 #define TU_CALLX(device, thing) FD_CALLX((device)->physical_device->info, thing)
diff --git a/src/freedreno/vulkan/tu_device.cc b/src/freedreno/vulkan/tu_device.cc
index b3e7b8bdafcb1..949efdaac7561 100644
--- a/src/freedreno/vulkan/tu_device.cc
+++ b/src/freedreno/vulkan/tu_device.cc
@@ -333,6 +333,7 @@ get_device_extensions(const struct tu_physical_device *device,
       .GOOGLE_user_type = true,
       .IMG_filter_cubic = device->info->a6xx.has_tex_filter_cubic,
       .NV_compute_shader_derivatives = device->info->chip >= 7,
+      .QCOM_fragment_density_map_offset = true,
       .VALVE_mutable_descriptor_type = true,
    } };
 
@@ -747,6 +748,9 @@ tu_get_features(struct tu_physical_device *pdevice,
    /* VK_KHR_subgroup_rotate */
    features->shaderSubgroupRotate = true;
    features->shaderSubgroupRotateClustered = true;
+
+   /* VK_QCOM_fragment_density_map_offset */
+   features->fragmentDensityMapOffset = true;
 }
 
 static void
@@ -1385,6 +1389,11 @@ tu_get_properties(struct tu_physical_device *pdevice,
    props->degenerateLinesRasterized = false;
    props->fullyCoveredFragmentShaderInputVariable = false;
    props->conservativeRasterizationPostDepthCoverage = false;
+
+   /* VK_QCOM_fragment_density_map_offset */
+   props->fragmentDensityOffsetGranularity = (VkExtent2D) { 
+      TU_FDM_OFFSET_GRANULARITY, TU_FDM_OFFSET_GRANULARITY
+   };
 }
 
 static const struct vk_pipeline_cache_object_ops *const cache_import_ops[] = {
diff --git a/src/freedreno/vulkan/tu_device.h b/src/freedreno/vulkan/tu_device.h
index e8277967a0bc5..fc9b898eea022 100644
--- a/src/freedreno/vulkan/tu_device.h
+++ b/src/freedreno/vulkan/tu_device.h
@@ -488,7 +488,7 @@ struct tu_tiling_config {
    /* Whether using GMEM is even possible with this configuration */
    bool possible;
 
-   struct tu_vsc_config vsc;
+   struct tu_vsc_config vsc, fdm_offset_vsc;
 };
 
 struct tu_framebuffer
diff --git a/src/freedreno/vulkan/tu_dynamic_rendering.cc b/src/freedreno/vulkan/tu_dynamic_rendering.cc
index 1ff7d4b631eb5..e7618b4728196 100644
--- a/src/freedreno/vulkan/tu_dynamic_rendering.cc
+++ b/src/freedreno/vulkan/tu_dynamic_rendering.cc
@@ -152,7 +152,7 @@ tu_insert_dynamic_cmdbufs(struct tu_device *dev,
                                         old_cmds[i]->pre_chain.trace_renderpass_end);
          }
 
-         TU_CALLX(dev, tu_cmd_render)(cmd_buffer);
+         TU_CALLX(dev, tu_cmd_render)(cmd_buffer, NULL);
 
          tu_cs_emit_pkt7(&cmd_buffer->cs, CP_MEM_WRITE, 3);
          tu_cs_emit_qw(&cmd_buffer->cs,
diff --git a/src/freedreno/vulkan/tu_image.cc b/src/freedreno/vulkan/tu_image.cc
index da5e1e520a4cf..16da996a4d0f6 100644
--- a/src/freedreno/vulkan/tu_image.cc
+++ b/src/freedreno/vulkan/tu_image.cc
@@ -1163,10 +1163,10 @@ tu_DestroyImageView(VkDevice _device,
  */
 void
 tu_fragment_density_map_sample(const struct tu_image_view *fdm,
-                               uint32_t x, uint32_t y,
+                               int32_t x, int32_t y,
                                uint32_t width, uint32_t height,
-                               uint32_t layers,
-                               struct tu_frag_area *areas)
+                               uint32_t layer,
+                               struct tu_frag_area *area)
 {
    assert(fdm->image->layout[0].tile_mode == TILE6_LINEAR);
 
@@ -1176,20 +1176,19 @@ tu_fragment_density_map_sample(const struct tu_image_view *fdm,
    fdm_shift_x = CLAMP(fdm_shift_x, MIN_FDM_TEXEL_SIZE_LOG2, MAX_FDM_TEXEL_SIZE_LOG2);
    fdm_shift_y = CLAMP(fdm_shift_y, MIN_FDM_TEXEL_SIZE_LOG2, MAX_FDM_TEXEL_SIZE_LOG2);
 
-   uint32_t i = x >> fdm_shift_x;
-   uint32_t j = y >> fdm_shift_y;
+   int32_t i = x >> fdm_shift_x;
+   int32_t j = y >> fdm_shift_y;
+
+   i = CLAMP(i, 0, fdm->vk.extent.width - 1);
+   j = CLAMP(j, 0, fdm->vk.extent.height - 1);
 
    unsigned cpp = fdm->image->layout[0].cpp;
    unsigned pitch = fdm->view.pitch;
 
-   void *pixel = (char *)fdm->image->map + fdm->view.offset + cpp * i + pitch * j;
-   for (unsigned i = 0; i < layers; i++) {
-      float density_src[4], density[4];
-      util_format_unpack_rgba(fdm->view.format, density_src, pixel, 1);
-      pipe_swizzle_4f(density, density_src, fdm->swizzle);
-      areas[i].width = 1.0f / density[0];
-      areas[i].height = 1.0f / density[1];
-
-      pixel = (char *)pixel + fdm->view.layer_size;
-   }
+   void *pixel = (char *)fdm->image->map + fdm->view.offset + fdm->view.layer_size * layer + cpp * i + pitch * j;
+   float density_src[4], density[4];
+   util_format_unpack_rgba(fdm->view.format, density_src, pixel, 1);
+   pipe_swizzle_4f(density, density_src, fdm->swizzle);
+   area->width = 1.0f / density[0];
+   area->height = 1.0f / density[1];
 }
diff --git a/src/freedreno/vulkan/tu_image.h b/src/freedreno/vulkan/tu_image.h
index 5d47327b7c985..6bfb48c7bc0d8 100644
--- a/src/freedreno/vulkan/tu_image.h
+++ b/src/freedreno/vulkan/tu_image.h
@@ -129,9 +129,9 @@ struct tu_frag_area {
 
 void
 tu_fragment_density_map_sample(const struct tu_image_view *fdm,
-                               uint32_t x, uint32_t y,
+                               int32_t x, int32_t y,
                                uint32_t width, uint32_t height,
-                               uint32_t layers, struct tu_frag_area *areas);
+                               uint32_t layer, struct tu_frag_area *area);
 
 VkResult
 tu_image_update_layout(struct tu_device *device, struct tu_image *image,
diff --git a/src/freedreno/vulkan/tu_lrz.cc b/src/freedreno/vulkan/tu_lrz.cc
index 877762e9483eb..69e380dbb195f 100644
--- a/src/freedreno/vulkan/tu_lrz.cc
+++ b/src/freedreno/vulkan/tu_lrz.cc
@@ -117,7 +117,7 @@ tu6_write_lrz_cntl(struct tu_cmd_buffer *cmd, struct tu_cs *cs,
                    struct A6XX_GRAS_LRZ_CNTL cntl)
 {
    if (CHIP >= A7XX) {
-      // A7XX split LRZ_CNTL into two seperate registers.
+      /* A7XX split LRZ_CNTL into two seperate registers. */
       struct tu_reg_value cntl2 = A7XX_GRAS_LRZ_CNTL2(
          .disable_on_wrong_dir = cntl.disable_on_wrong_dir,
          .fc_enable = cntl.fc_enable,
diff --git a/src/freedreno/vulkan/tu_pipeline.cc b/src/freedreno/vulkan/tu_pipeline.cc
index 7352886e767f2..19abd36327954 100644
--- a/src/freedreno/vulkan/tu_pipeline.cc
+++ b/src/freedreno/vulkan/tu_pipeline.cc
@@ -2545,44 +2545,49 @@ struct apply_viewport_state {
    bool share_scale;
 };
 
-/* It's a hardware restriction that the window offset (i.e. bin.offset) must
- * be the same for all views. This means that GMEM coordinates cannot be a
- * simple scaling of framebuffer coordinates, because this would require us to
- * scale the window offset and the scale may be different per view. Instead we
- * have to apply a per-bin offset to the GMEM coordinate transform to make
- * sure that the window offset maps to itself. Specifically we need an offset
- * o to the transform:
+/* It's a hardware restriction that the window offset (i.e. common_bin_offset)
+ * must be the same for all views. This means that GMEM coordinates cannot be
+ * a simple scaling of framebuffer coordinates, because this would require us
+ * to scale the window offset and the scale may be different per view. Instead
+ * we have to apply a per-bin offset to the GMEM coordinate transform to make
+ * sure that the window offset maps to the per-view bin coordinate, which will
+ * be the same if there is no offset. Specifically we need an offset o to the
+ * transform:
  *
  * x' = s * x + o
  *
- * so that when we plug in the bin start b_s:
+ * so that when we plug in the per-view bin start b_s and the common window
+ * offset b_cs:
  * 
- * b_s = s * b_s + o
+ * b_cs = s * b_s + o
  *
  * and we get:
  *
- * o = b_s - s * b_s
+ * o = b_cs - s * b_s
  *
- * We use this form exactly, because we know the bin offset is a multiple of
+ * We use this form exactly, because we know the bin start is a multiple of
  * the frag area so s * b_s is an integer and we can compute an exact result
- * easily.
+ * easily. We also have to make sure that the bin offset is a multiple of the
+ * frag area by restricting the frag area.
  */
 
 VkOffset2D
-tu_fdm_per_bin_offset(VkExtent2D frag_area, VkRect2D bin)
+tu_fdm_per_bin_offset(VkExtent2D frag_area, VkRect2D bin,
+                      VkOffset2D common_bin_offset)
 {
    assert(bin.offset.x % frag_area.width == 0);
    assert(bin.offset.y % frag_area.height == 0);
 
    return (VkOffset2D) {
-      bin.offset.x - bin.offset.x / frag_area.width,
-      bin.offset.y - bin.offset.y / frag_area.height
+      common_bin_offset.x - bin.offset.x / frag_area.width,
+      common_bin_offset.y - bin.offset.y / frag_area.height
    };
 }
 
 static void
 fdm_apply_viewports(struct tu_cmd_buffer *cmd, struct tu_cs *cs, void *data,
-                    VkRect2D bin, unsigned views, const VkExtent2D *frag_areas)
+                    VkOffset2D common_bin_offset, unsigned views,
+                    const VkExtent2D *frag_areas, const VkRect2D *bins)
 {
    const struct apply_viewport_state *state =
       (const struct apply_viewport_state *)data;
@@ -2600,9 +2605,12 @@ fdm_apply_viewports(struct tu_cmd_buffer *cmd, struct tu_cs *cs, void *data,
        * replicate it across all viewports.
        */
       VkExtent2D frag_area = state->share_scale ? frag_areas[0] : frag_areas[i];
+      VkRect2D bin = state->share_scale ? bins[0] : bins[i];
       VkViewport viewport =
          state->share_scale ? state->vp.viewports[i] : state->vp.viewports[0];
-      if (frag_area.width == 1 && frag_area.height == 1) {
+      if (frag_area.width == 1 && frag_area.height == 1 &&
+          common_bin_offset.x == bin.offset.x &&
+          common_bin_offset.y == bin.offset.y) {
          vp.viewports[i] = viewport;
          continue;
       }
@@ -2615,7 +2623,8 @@ fdm_apply_viewports(struct tu_cmd_buffer *cmd, struct tu_cs *cs, void *data,
       vp.viewports[i].width = viewport.width * scale_x;
       vp.viewports[i].height = viewport.height * scale_y;
 
-      VkOffset2D offset = tu_fdm_per_bin_offset(frag_area, bin);
+      VkOffset2D offset = tu_fdm_per_bin_offset(frag_area, bin,
+                                                common_bin_offset);
 
       vp.viewports[i].x = scale_x * viewport.x + offset.x;
       vp.viewports[i].y = scale_y * viewport.y + offset.y;
@@ -2691,7 +2700,8 @@ tu6_emit_scissor(struct tu_cs *cs, const struct vk_viewport_state *vp)
 
 static void
 fdm_apply_scissors(struct tu_cmd_buffer *cmd, struct tu_cs *cs, void *data,
-                   VkRect2D bin, unsigned views, const VkExtent2D *frag_areas)
+                   VkOffset2D common_bin_offset, unsigned views,
+                   const VkExtent2D *frag_areas, const VkRect2D *bins)
 {
    const struct apply_viewport_state *state =
       (const struct apply_viewport_state *)data;
@@ -2700,12 +2710,9 @@ fdm_apply_scissors(struct tu_cmd_buffer *cmd, struct tu_cs *cs, void *data,
 
    for (unsigned i = 0; i < vp.scissor_count; i++) {
       VkExtent2D frag_area = state->share_scale ? frag_areas[0] : frag_areas[i];
+      VkRect2D bin = state->share_scale ? bins[0] : bins[i];
       VkRect2D scissor =
          state->share_scale ? state->vp.scissors[i] : state->vp.scissors[0];
-      if (frag_area.width == 1 && frag_area.height == 1) {
-         vp.scissors[i] = scissor;
-         continue;
-      }
 
       /* Transform the scissor following the viewport. It's unclear how this
        * is supposed to handle cases where the scissor isn't aligned to the
@@ -2713,7 +2720,8 @@ fdm_apply_scissors(struct tu_cmd_buffer *cmd, struct tu_cs *cs, void *data,
        * fragments if the scissor size equals the framebuffer size and it
        * isn't aligned to the fragment area.
        */
-      VkOffset2D offset = tu_fdm_per_bin_offset(frag_area, bin);
+      VkOffset2D offset = tu_fdm_per_bin_offset(frag_area, bin,
+                                                common_bin_offset);
       VkOffset2D min = {
          scissor.offset.x / frag_area.width + offset.x,
          scissor.offset.y / frag_area.width + offset.y,
@@ -2728,12 +2736,12 @@ fdm_apply_scissors(struct tu_cmd_buffer *cmd, struct tu_cs *cs, void *data,
        */
       uint32_t scaled_width = bin.extent.width / frag_area.width;
       uint32_t scaled_height = bin.extent.height / frag_area.height;
-      vp.scissors[i].offset.x = MAX2(min.x, bin.offset.x);
-      vp.scissors[i].offset.y = MAX2(min.y, bin.offset.y);
+      vp.scissors[i].offset.x = MAX2(min.x, common_bin_offset.x);
+      vp.scissors[i].offset.y = MAX2(min.y, common_bin_offset.y);
       vp.scissors[i].extent.width =
-         MIN2(max.x, bin.offset.x + scaled_width) - vp.scissors[i].offset.x;
+         MIN2(max.x, common_bin_offset.x + scaled_width) - vp.scissors[i].offset.x;
       vp.scissors[i].extent.height =
-         MIN2(max.y, bin.offset.y + scaled_height) - vp.scissors[i].offset.y;
+         MIN2(max.y, common_bin_offset.y + scaled_height) - vp.scissors[i].offset.y;
    }
 
    TU_CALLX(cs->device, tu6_emit_scissor)(cs, &vp);
diff --git a/src/freedreno/vulkan/tu_pipeline.h b/src/freedreno/vulkan/tu_pipeline.h
index 5499b58bd3872..0c0be7973f408 100644
--- a/src/freedreno/vulkan/tu_pipeline.h
+++ b/src/freedreno/vulkan/tu_pipeline.h
@@ -236,7 +236,8 @@ TU_DECL_PIPELINE_DOWNCAST(graphics, TU_PIPELINE_GRAPHICS)
 TU_DECL_PIPELINE_DOWNCAST(graphics_lib, TU_PIPELINE_GRAPHICS_LIB)
 TU_DECL_PIPELINE_DOWNCAST(compute, TU_PIPELINE_COMPUTE)
 
-VkOffset2D tu_fdm_per_bin_offset(VkExtent2D frag_area, VkRect2D bin);
+VkOffset2D tu_fdm_per_bin_offset(VkExtent2D frag_area, VkRect2D bin,
+                                 VkOffset2D common_bin_offset);
 
 template <chip CHIP>
 uint32_t tu_emit_draw_state(struct tu_cmd_buffer *cmd);
diff --git a/src/freedreno/vulkan/tu_util.cc b/src/freedreno/vulkan/tu_util.cc
index 07e13dd88811e..fc4eed4c9559e 100644
--- a/src/freedreno/vulkan/tu_util.cc
+++ b/src/freedreno/vulkan/tu_util.cc
@@ -49,6 +49,7 @@ static const struct debug_control tu_debug_options[] = {
    { "dumpas", TU_DEBUG_DUMPAS },
    { "nobinmerging", TU_DEBUG_NO_BIN_MERGING },
    { "perfcraw", TU_DEBUG_PERFCRAW },
+   { "fdmoffset", TU_DEBUG_FDM_OFFSET },
    { NULL, 0 }
 };
 
@@ -449,6 +450,16 @@ tu_framebuffer_tiling_config(struct tu_framebuffer *fb,
       tu_tiling_config_update_pipe_layout(vsc, device, pass->has_fdm);
       tu_tiling_config_update_pipes(vsc, device);
       tu_tiling_config_update_binning(vsc, device);
+
+      if (pass->has_fdm) {
+         struct tu_vsc_config *fdm_offset_vsc = &tiling->fdm_offset_vsc;
+         fdm_offset_vsc->tile_count = (VkExtent2D) {
+            vsc->tile_count.width + 1, vsc->tile_count.height + 1
+         };
+         tu_tiling_config_update_pipe_layout(fdm_offset_vsc, device, true);
+         tu_tiling_config_update_pipes(fdm_offset_vsc, device);
+         tu_tiling_config_update_binning(fdm_offset_vsc, device);
+      }
    }
 }
 
diff --git a/src/freedreno/vulkan/tu_util.h b/src/freedreno/vulkan/tu_util.h
index 173c8ace984ec..5ebdcd26a9d9f 100644
--- a/src/freedreno/vulkan/tu_util.h
+++ b/src/freedreno/vulkan/tu_util.h
@@ -69,6 +69,7 @@ enum tu_debug_flags : uint64_t
    TU_DEBUG_DUMPAS                   = BITFIELD64_BIT(28),
    TU_DEBUG_NO_BIN_MERGING           = BITFIELD64_BIT(29),
    TU_DEBUG_PERFCRAW                 = BITFIELD64_BIT(30),
+   TU_DEBUG_FDM_OFFSET               = BITFIELD64_BIT(31),
 };
 
 struct tu_env {
-- 
GitLab

From c4ed1d993f516052d9f887bef2054024044e5e78 Mon Sep 17 00:00:00 2001
From: Connor Abbott <cwabbott0@gmail.com>
Date: Mon, 17 Feb 2025 17:11:23 -0500
Subject: [PATCH 10/12] vk/runtime: Use vk_command_buffer in renderpass
 wrappers

The comment is out of date, and all drivers using the runtime use
vk_command_buffer. Let's use it directly.
---
 src/vulkan/runtime/vk_render_pass.c | 25 ++++++++-----------------
 1 file changed, 8 insertions(+), 17 deletions(-)

diff --git a/src/vulkan/runtime/vk_render_pass.c b/src/vulkan/runtime/vk_render_pass.c
index 121f9b1f85cc5..40c618ea7ed09 100644
--- a/src/vulkan/runtime/vk_render_pass.c
+++ b/src/vulkan/runtime/vk_render_pass.c
@@ -248,43 +248,34 @@ vk_common_CmdBeginRenderPass(VkCommandBuffer commandBuffer,
                              const VkRenderPassBeginInfo* pRenderPassBegin,
                              VkSubpassContents contents)
 {
-   /* We don't have a vk_command_buffer object but we can assume, since we're
-    * using common dispatch, that it's a vk_object of some sort.
-    */
-   struct vk_object_base *disp = (struct vk_object_base *)commandBuffer;
+   VK_FROM_HANDLE(vk_command_buffer, cmd_buffer, commandBuffer);
 
    VkSubpassBeginInfo info = {
       .sType = VK_STRUCTURE_TYPE_SUBPASS_BEGIN_INFO,
       .contents = contents,
    };
 
-   disp->device->dispatch_table.CmdBeginRenderPass2(commandBuffer,
-                                                    pRenderPassBegin, &info);
+   cmd_buffer->base.device->dispatch_table.CmdBeginRenderPass2(
+      commandBuffer, pRenderPassBegin, &info);
 }
 
 VKAPI_ATTR void VKAPI_CALL
 vk_common_CmdEndRenderPass(VkCommandBuffer commandBuffer)
 {
-   /* We don't have a vk_command_buffer object but we can assume, since we're
-    * using common dispatch, that it's a vk_object of some sort.
-    */
-   struct vk_object_base *disp = (struct vk_object_base *)commandBuffer;
+   VK_FROM_HANDLE(vk_command_buffer, cmd_buffer, commandBuffer);
 
    VkSubpassEndInfo info = {
       .sType = VK_STRUCTURE_TYPE_SUBPASS_END_INFO,
    };
 
-   disp->device->dispatch_table.CmdEndRenderPass2(commandBuffer, &info);
+   cmd_buffer->base.device->dispatch_table.CmdEndRenderPass2(commandBuffer, &info);
 }
 
 VKAPI_ATTR void VKAPI_CALL
 vk_common_CmdNextSubpass(VkCommandBuffer commandBuffer,
                          VkSubpassContents contents)
 {
-   /* We don't have a vk_command_buffer object but we can assume, since we're
-    * using common dispatch, that it's a vk_object of some sort.
-    */
-   struct vk_object_base *disp = (struct vk_object_base *)commandBuffer;
+   VK_FROM_HANDLE(vk_command_buffer, cmd_buffer, commandBuffer);
 
    VkSubpassBeginInfo begin_info = {
       .sType = VK_STRUCTURE_TYPE_SUBPASS_BEGIN_INFO,
@@ -295,8 +286,8 @@ vk_common_CmdNextSubpass(VkCommandBuffer commandBuffer,
       .sType = VK_STRUCTURE_TYPE_SUBPASS_END_INFO,
    };
 
-   disp->device->dispatch_table.CmdNextSubpass2(commandBuffer, &begin_info,
-                                                &end_info);
+   cmd_buffer->base.device->dispatch_table.CmdNextSubpass2(
+      commandBuffer, &begin_info, &end_info);
 }
 
 static unsiged
Gitab


From 1e2f970c509006cbfd0c015181d64748ef023597 Mon Sep 17 00:00:00 2001
From: Connor Abbott <cwabbott0@gmail.com>
Date: Mon, 17 Feb 2025 17:16:08 -0500
Subject: [PATCH 11/12] vk/runtime: Add common CmdEndRendering

Similar to the common CmdEndRenderPass, add a default implementation for
drivers that implement VK_EXT_fragment_density_map_offset.
---
 src/vulkan/runtime/vk_render_pass.c | 12 ++++++++++++
 1 file changed, 12 insertions(+)

diff --git a/src/vulkan/runtime/vk_render_pass.c b/src/vulkan/runtime/vk_render_pass.c
index 40c618ea7ed09..97733274e6069 100644
--- a/src/vulkan/runtime/vk_render_pass.c
+++ b/src/vulkan/runtime/vk_render_pass.c
@@ -290,6 +290,18 @@ vk_common_CmdNextSubpass(VkCommandBuffer commandBuffer,
       commandBuffer, &begin_info, &end_info);
 }
 
+VKAPI_ATTR void VKAPI_CALL
+vk_common_CmdEndRendering(VkCommandBuffer commandBuffer)
+{
+   VK_FROM_HANDLE(vk_command_buffer, cmd_buffer, commandBuffer);
+
+   VkRenderingEndInfoEXT info = {
+      .sType = VK_STRUCTURE_TYPE_RENDERING_END_INFO_EXT,
+   };
+
+   cmd_buffer->base.device->dispatch_table.CmdEndRendering2EXT(commandBuffer, &info);
+}
+
 static unsigned
 num_subpass_attachments2(const VkSubpassDescription2 *desc)
 {
-- 
GitLab


From 99269786fcc93a4042e9336d03d333d1004744da Mon Sep 17 00:00:00 2001
From: Connor Abbott <cwabbott0@gmail.com>
Date: Mon, 17 Feb 2025 17:21:36 -0500
Subject: [PATCH 12/12] tu: Implement VK_EXT_fragment_density_map_offset

Implement support for dynamic rendering, including suspending and
resuming render passes.
---
 src/freedreno/vulkan/tu_cmd_buffer.cc        | 55 ++++++++++++++------
 src/freedreno/vulkan/tu_cmd_buffer.h         |  3 ++
 src/freedreno/vulkan/tu_device.cc            |  3 +-
 src/freedreno/vulkan/tu_dynamic_rendering.cc |  5 +-
 4 files changed, 49 insertions(+), 17 deletions(-)

diff --git a/src/freedreno/vulkan/tu_cmd_buffer.cc b/src/freedreno/vulkan/tu_cmd_buffer.cc
index 6cd9b01759f19..886fdf53bc4d7 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.cc
+++ b/src/freedreno/vulkan/tu_cmd_buffer.cc
@@ -1759,7 +1759,7 @@ tu_enable_fdm_offset(struct tu_cmd_buffer *cmd)
 
    const struct tu_image_view *fdm = cmd->state.attachments[fdm_a];
    return fdm->image->vk.create_flags &
-      VK_IMAGE_CREATE_FRAGMENT_DENSITY_MAP_OFFSET_BIT_QCOM;
+      VK_IMAGE_CREATE_FRAGMENT_DENSITY_MAP_OFFSET_BIT_EXT;
 }
 
 static void
@@ -4813,6 +4813,13 @@ tu_append_pre_chain(struct tu_cmd_buffer *cmd,
          secondary->pre_chain.trace_renderpass_end);
    util_dynarray_append_dynarray(&cmd->fdm_bin_patchpoints,
                                  &secondary->pre_chain.fdm_bin_patchpoints);
+
+   cmd->pre_chain.fdm_offset = secondary->pre_chain.fdm_offset;
+   if (secondary->pre_chain.fdm_offset) {
+      memcpy(cmd->pre_chain.fdm_offsets,
+             secondary->pre_chain.fdm_offsets,
+             sizeof(cmd->pre_chain.fdm_offsets));
+   }
 }
 
 /* Take the saved post-chain in "secondary" and copy it to "cmd".
@@ -4983,7 +4990,7 @@ tu_CmdExecuteCommands(VkCommandBuffer commandBuffer,
                   cmd->state.suspend_resume = SR_AFTER_PRE_CHAIN;
                   break;
                case SR_IN_CHAIN:
-               case SR_IN_CHAIN_AFTER_PRE_CHAIN:
+               case SR_IN_CHAIN_AFTER_PRE_CHAIN: {
                   /* The renderpass ends in the secondary and starts somewhere
                    * earlier in this primary. Since the last render pass in
                    * the chain is in the secondary, we are technically outside
@@ -4993,12 +5000,16 @@ tu_CmdExecuteCommands(VkCommandBuffer commandBuffer,
                    */
                   tu_restore_suspended_pass(cmd, cmd);
 
-                  TU_CALLX(cmd->device, tu_cmd_render)(cmd, NULL);
+                  const struct VkOffset2D *fdm_offsets =
+                     cmd->pre_chain.fdm_offset ?
+                     cmd->pre_chain.fdm_offsets : NULL;
+                  TU_CALLX(cmd->device, tu_cmd_render)(cmd, fdm_offsets);
                   if (cmd->state.suspend_resume == SR_IN_CHAIN)
                      cmd->state.suspend_resume = SR_NONE;
                   else
                      cmd->state.suspend_resume = SR_AFTER_PRE_CHAIN;
                   break;
+               }
                case SR_AFTER_PRE_CHAIN:
                   unreachable("resuming render pass is not preceded by suspending one");
                }
@@ -7661,9 +7672,9 @@ tu_CmdEndRenderPass2(VkCommandBuffer commandBuffer,
       return;
    }
 
-   const VkSubpassFragmentDensityMapOffsetEndInfoQCOM *fdm_offset_info =
+   const VkRenderPassFragmentDensityMapOffsetEndInfoEXT *fdm_offset_info =
       vk_find_struct_const(pSubpassEndInfo->pNext,
-                           SUBPASS_FRAGMENT_DENSITY_MAP_OFFSET_END_INFO_QCOM);
+                           RENDER_PASS_FRAGMENT_DENSITY_MAP_OFFSET_END_INFO_EXT);
    const VkOffset2D *fdm_offsets =
       (fdm_offset_info && fdm_offset_info->fragmentDensityOffsetCount > 0) ?
       fdm_offset_info->pFragmentDensityOffsets : NULL;
@@ -7691,7 +7702,8 @@ tu_CmdEndRenderPass2(VkCommandBuffer commandBuffer,
 }
 
 VKAPI_ATTR void VKAPI_CALL
-tu_CmdEndRendering(VkCommandBuffer commandBuffer)
+tu_CmdEndRendering2EXT(VkCommandBuffer commandBuffer,
+                       const VkRenderingEndInfoEXT *pRenderingEndInfo)
 {
    VK_FROM_HANDLE(tu_cmd_buffer, cmd_buffer, commandBuffer);
 
@@ -7704,6 +7716,22 @@ tu_CmdEndRendering(VkCommandBuffer commandBuffer)
          (cmd_buffer, &cmd_buffer->draw_cs);
    }
 
+   const VkRenderPassFragmentDensityMapOffsetEndInfoEXT *fdm_offset_info =
+      vk_find_struct_const(pRenderingEndInfo->pNext,
+                           RENDER_PASS_FRAGMENT_DENSITY_MAP_OFFSET_END_INFO_EXT);
+   const VkOffset2D *fdm_offsets =
+      (fdm_offset_info && fdm_offset_info->fragmentDensityOffsetCount > 0) ?
+      fdm_offset_info->pFragmentDensityOffsets : NULL;
+
+   VkOffset2D test_offsets[MAX_VIEWS];
+   if (TU_DEBUG(FDM) && TU_DEBUG(FDM_OFFSET)) {
+      for (unsigned i = 0;
+           i < MAX2(cmd_buffer->state.pass->num_views, 1); i++) {
+         test_offsets[i] = { 64, 64 };
+      }
+      fdm_offsets = test_offsets;
+   }
+
    if (!cmd_buffer->state.suspending) {
       tu_cs_end(&cmd_buffer->draw_cs);
       tu_cs_end(&cmd_buffer->draw_epilogue_cs);
@@ -7711,21 +7739,18 @@ tu_CmdEndRendering(VkCommandBuffer commandBuffer)
       if (cmd_buffer->state.suspend_resume == SR_IN_PRE_CHAIN) {
          cmd_buffer->trace_renderpass_end = u_trace_end_iterator(&cmd_buffer->trace);
          tu_save_pre_chain(cmd_buffer);
+         cmd_buffer->pre_chain.fdm_offset = !!fdm_offsets;
+         if (fdm_offsets) {
+            memcpy(cmd_buffer->pre_chain.fdm_offsets,
+                   fdm_offsets, sizeof(VkOffset2D) *
+                   MAX2(cmd_buffer->state.pass->num_views, 1));
+         }
 
          /* Even we don't call tu_cmd_render here, renderpass is finished
           * and draw states should be disabled.
           */
          tu_disable_draw_states(cmd_buffer, &cmd_buffer->cs);
       } else {
-         VkOffset2D test_offsets[MAX_VIEWS];
-         const VkOffset2D *fdm_offsets = NULL;
-         if (TU_DEBUG(FDM) && TU_DEBUG(FDM_OFFSET)) {
-            for (unsigned i = 0;
-                 i < MAX2(cmd_buffer->state.pass->num_views, 1); i++) {
-               test_offsets[i] = { 64, 64 };
-            }
-            fdm_offsets = test_offsets;
-         }
          TU_CALLX(cmd_buffer->device, tu_cmd_render)(cmd_buffer, fdm_offsets);
       }
 
diff --git a/src/freedreno/vulkan/tu_cmd_buffer.h b/src/freedreno/vulkan/tu_cmd_buffer.h
index b6e7d4d6c91f9..301cfa64d98a3 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.h
+++ b/src/freedreno/vulkan/tu_cmd_buffer.h
@@ -615,6 +615,9 @@ struct tu_cmd_buffer
       struct tu_cs draw_cs;
       struct tu_cs draw_epilogue_cs;
 
+      bool fdm_offset;
+      VkOffset2D fdm_offsets[MAX_VIEWS];
+
       struct u_trace_iterator trace_renderpass_start, trace_renderpass_end;
 
       struct tu_render_pass_state state;
diff --git a/src/freedreno/vulkan/tu_device.cc b/src/freedreno/vulkan/tu_device.cc
index 949efdaac7561..b6776ee2b63a2 100644
--- a/src/freedreno/vulkan/tu_device.cc
+++ b/src/freedreno/vulkan/tu_device.cc
@@ -273,6 +273,7 @@ get_device_extensions(const struct tu_physical_device *device,
       .EXT_external_memory_dma_buf = true,
       .EXT_filter_cubic = device->info->a6xx.has_tex_filter_cubic,
       .EXT_fragment_density_map = true,
+      .EXT_fragment_density_map_offset = true,
       .EXT_global_priority = true,
       .EXT_global_priority_query = true,
       .EXT_graphics_pipeline_library = true,
@@ -749,7 +750,7 @@ tu_get_features(struct tu_physical_device *pdevice,
    features->shaderSubgroupRotate = true;
    features->shaderSubgroupRotateClustered = true;
 
-   /* VK_QCOM_fragment_density_map_offset */
+   /* VK_EXT_fragment_density_map_offset */
    features->fragmentDensityMapOffset = true;
 }
 
diff --git a/src/freedreno/vulkan/tu_dynamic_rendering.cc b/src/freedreno/vulkan/tu_dynamic_rendering.cc
index e7618b4728196..4d258b68fd346 100644
--- a/src/freedreno/vulkan/tu_dynamic_rendering.cc
+++ b/src/freedreno/vulkan/tu_dynamic_rendering.cc
@@ -152,7 +152,10 @@ tu_insert_dynamic_cmdbufs(struct tu_device *dev,
                                         old_cmds[i]->pre_chain.trace_renderpass_end);
          }
 
-         TU_CALLX(dev, tu_cmd_render)(cmd_buffer, NULL);
+         const struct VkOffset2D *fdm_offsets =
+            cmd_buffer->pre_chain.fdm_offset ?
+            cmd_buffer->pre_chain.fdm_offsets : NULL;
+         TU_CALLX(dev, tu_cmd_render)(cmd_buffer, fdm_offsets);
 
          tu_cs_emit_pkt7(&cmd_buffer->cs, CP_MEM_WRITE, 3);
          tu_cs_emit_qw(&cmd_buffer->cs,
-- 
GitLab

